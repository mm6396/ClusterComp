{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U_J6KpxiyLut"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#Method for Preprocessing datasetsand replacing nulls :\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "def processing_procedure(files):\n",
        "    if not os.path.exists(\"precessed_datasets\"):\n",
        "        os.mkdir(\"precessed_datasets\")\n",
        "    processed_files = []\n",
        "    for filename in files:\n",
        "        processed_filename = \"precessed_datasets/\"+filename.split('.')[0] + '_processed.' + filename.split('.')[1]\n",
        "        if os.path.exists(processed_filename):\n",
        "            print(f\"{processed_filename} already exists.\")\n",
        "        else :\n",
        "            df = pd.read_excel(\"datasets/\"+filename, engine='openpyxl')\n",
        "\n",
        "            # Interpolate using the nearest method\n",
        "            df.interpolate(method='nearest', inplace=True, limit_direction='both')\n",
        "            \n",
        "            # Fill remaining NaNs with the mean of the column\n",
        "            df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "            \n",
        "\n",
        "            \n",
        "\n",
        "            # Save (or overwrite) the processed data into the new spreadsheet.\n",
        "            df.to_excel(processed_filename, index=False)\n",
        "\n",
        "        processed_files.append(processed_filename)\n",
        "\n",
        "    return processed_files\n",
        "\n",
        "def print_rankings(dist, aggr,cols):\n",
        "    print(\"A Kemeny-Young aggregation with score {} is:\\n {}\".format(\n",
        "    dist,\n",
        "    \"\\n \".join(cols[i] for i in np.argsort(aggr))))\n",
        "    \n",
        "    \n",
        "def get_array_rankings(aggr, cols):\n",
        "    rankings = [cols[i] for i in np.argsort(aggr)]\n",
        "    return rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P-BHYsi67xZ3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import combinations, permutations\n",
        "\n",
        "\n",
        "\n",
        "def kendalltau_dist(rank_a, rank_b):\n",
        "    tau = 0\n",
        "    n_candidates = len(rank_a)\n",
        "    for i, j in combinations(range(n_candidates), 2):\n",
        "        tau += (np.sign(rank_a[i] - rank_a[j]) ==\n",
        "                -np.sign(rank_b[i] - rank_b[j]))\n",
        "    return tau\n",
        "\n",
        "\n",
        "def rankaggr_brute(ranks):\n",
        "    min_dist = np.inf\n",
        "    best_rank = None\n",
        "    n_voters, n_candidates = ranks.shape\n",
        "    for candidate_rank in permutations(range(n_candidates)):\n",
        "        distances=[kendalltau_dist(candidate_rank, rank) for rank in ranks]\n",
        "        dist = np.sum(distances)\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            best_rank = candidate_rank\n",
        "    return min_dist, best_rank\n",
        "\n",
        "\n",
        "def parse_rankings(file_path):\n",
        "    rankings = []\n",
        "    cols=[]\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                ranking_str = line.split(':')[1].strip()\n",
        "                ranking_values = [col[1:-1] for col in ranking_str.split()]\n",
        "                if cols==[]:\n",
        "                    cols=ranking_values\n",
        "                    rankings.append(list(range(1,len(ranking_values)+1)))\n",
        "                else:\n",
        "                    rankings.append([cols.index(temp) for temp in ranking_values])\n",
        "    return np.array(rankings),cols\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MrETg3PbBaDJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precessed_datasets/test1_processed.xlsx already exists.\n",
            "precessed_datasets/test2_processed.xlsx already exists.\n",
            "precessed_datasets/test3_processed.xlsx already exists.\n",
            "precessed_datasets/test4_processed.xlsx already exists.\n",
            "NaN values in precessed_datasets/test1_processed.xlsx: 0\n",
            "NaN values in precessed_datasets/test2_processed.xlsx: 0\n",
            "NaN values in precessed_datasets/test3_processed.xlsx: 0\n",
            "NaN values in precessed_datasets/test4_processed.xlsx: 0\n"
          ]
        }
      ],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for \"Agglomerative\"\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import os,csv\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "param_grid = {\n",
        "    'n_clusters': [3],\n",
        "    'affinity': ['euclidean', 'manhattan'],\n",
        "    'linkage': ['ward', 'complete', 'average']\n",
        "}\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "for file in files_p:\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "    print(f\"NaN values in {file}:\", df.isnull().sum().sum())\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness' , 'label']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "# Mapping of files to the position we're interested in\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "for idx, file in enumerate(files_p):\n",
        "    if not os.path.exists(\"eval_datasets\"):\n",
        "        os.mkdir(\"eval_datasets\")\n",
        "\n",
        "    if not os.path.exists(\"tuning_datasets\"):\n",
        "        os.mkdir(\"tuning_datasets\")\n",
        "\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "\n",
        "    eval_filename = \"eval_datasets/\"+ f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "    # tuning\n",
        "    position = position_mapping[idx]\n",
        "    tuning_data = df[df[position] > 125]\n",
        "\n",
        "\n",
        "    tuning_filename =\"tuning_datasets/\"+ f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "\n",
        "\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for n_clusters in param_grid['n_clusters']:\n",
        "        for affinity in param_grid['affinity']:\n",
        "            for linkage in param_grid['linkage']:\n",
        "                # 'ward' can only work with 'euclidean'\n",
        "                if linkage == 'ward' and affinity != 'euclidean':\n",
        "                    continue\n",
        "\n",
        "                model = AgglomerativeClustering(n_clusters=n_clusters, affinity= affinity, linkage=linkage)\n",
        "                labels = model.fit_predict(scaled_eval)\n",
        "\n",
        "                score = silhouette_score(scaled_eval, labels)\n",
        "\n",
        "                # Append results to the list\n",
        "                results.append({\n",
        "                    'file': eval_filename,\n",
        "                    'n_clusters': n_clusters,\n",
        "                    'affinity': affinity,\n",
        "                    'linkage': linkage,\n",
        "                    'silhouette_score': score\n",
        "                })\n",
        "\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:4].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:4], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "if not os.path.exists(\"parameter_list\"):\n",
        "        os.mkdir(\"parameter_list\")\n",
        "\n",
        "output_file = \"parameter_list/Agglomerative+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "if not os.path.exists(\"rank_list\"):\n",
        "        os.mkdir(\"rank_list\")\n",
        "with open('rank_list/tuning_list_kemeny_agglomerative.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qjzcRfU53y6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Kemeny-Young aggregation with score 6 is:\n",
            " 3,euclidean,average\n",
            " 3,manhattan,complete\n",
            " 3,manhattan,average\n",
            " 3,euclidean,complete\n",
            " 3,euclidean,ward\n",
            "['3,euclidean,average', '3,manhattan,complete', '3,manhattan,average', '3,euclidean,complete', '3,euclidean,ward']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "file_path = \"rank_list/tuning_list_kemeny_agglomerative.txt\" \n",
        "rankings,cols = parse_rankings(file_path)\n",
        "\n",
        "dist, aggr = rankaggr_brute(rankings)\n",
        "\n",
        "print_rankings(dist, aggr,cols)\n",
        "Tuning_Agglo = get_array_rankings(aggr,cols)\n",
        "print(Tuning_Agglo)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
