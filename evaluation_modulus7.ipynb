{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mm6396/ClusterComp/blob/main/evaluation_modulus7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHucI84mXoGi"
      },
      "source": [
        "##**Imports Section**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***New Expremiments; ***"
      ],
      "metadata": {
        "id": "0UPGXTKg1FnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Method for Preprocessing datasetsand replacing nulls :\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def processing_procedure(files):\n",
        "    processed_files = []\n",
        "    for filename in files:\n",
        "\n",
        "        df = pd.read_excel(filename)\n",
        "\n",
        "\n",
        "        df.interpolate(method='nearest', inplace=True, limit_direction='both')\n",
        "\n",
        "\n",
        "        processed_filename = filename.split('.')[0] + '_processed.' + filename.split('.')[1]\n",
        "\n",
        "\n",
        "        if os.path.exists(processed_filename):\n",
        "            print(f\"{processed_filename} already exists. Overwriting...\")\n",
        "\n",
        "        # Save (or overwrite) the processed data into the new spreadsheet.\n",
        "        df.to_excel(processed_filename, index=False)\n",
        "\n",
        "        processed_files.append(processed_filename)\n",
        "\n",
        "    return processed_files\n"
      ],
      "metadata": {
        "id": "U_J6KpxiyLut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['HARDNESS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Hardness')\n",
        "\n",
        "plt.suptitle('Hardness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p5RBL3wOZ5EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['MODULUS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Modulus')\n",
        "\n",
        "plt.suptitle('Modulus based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6H0QYln6a75U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Stiffness'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Stiffness')\n",
        "plt.suptitle('Stiffness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jA0by-6JbK3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Load'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Load')\n",
        "\n",
        "plt.suptitle('Load based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tJll6_92be9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulizing Data\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop('Depth', axis=1) for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering methods\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3 , linkage = 'complete'), #linkage = {'ward' , 'complete' , 'average' , 'single'}\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "# Visualization\n",
        "n_rows = len(scaled_datasets)\n",
        "n_cols = len(clustering_algorithms)\n",
        "\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 15))\n",
        "\n",
        "for row, scaled_data in enumerate(scaled_datasets):\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for col, (algorithm_name, algorithm) in enumerate(clustering_algorithms.items()):\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        axs[row, col].scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        axs[row, col].set_title(f'Dataset {row + 1} using {algorithm_name}')\n",
        "\n",
        "\n",
        "        if row == 0:\n",
        "            axs[row, col].set_xlabel(algorithm_name)\n",
        "        if col == 0:\n",
        "            axs[row, col].set_ylabel(f'Dataset {row + 1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7BGrLWvwdHkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means"
      ],
      "metadata": {
        "id": "yRxJH-qcoQTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters (tuning parametets with GridSearch)\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering, Birch, KMeans\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Define the list of files\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering algorithms\n",
        "clustering_algorithms = {\n",
        "    'Agglomerative': AgglomerativeClustering(),\n",
        "    'birch': Birch(),\n",
        "    'KMeans': KMeans(),\n",
        "    'KMedoids': KMedoids()\n",
        "}\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grid_agglomerative = {\n",
        "    'n_clusters': [3],\n",
        "    'affinity': ['euclidean', 'manhattan'],\n",
        "    'linkage': ['ward', 'complete', 'average']\n",
        "}\n",
        "\n",
        "param_grid_birch = {\n",
        "    'threshold': [0.1, 0.3, 0.5],\n",
        "    'branching_factor': [50, 100, 200],\n",
        "    'n_clusters': [2, 3, 4, 5],\n",
        "    'compute_labels': [True, False]\n",
        "}\n",
        "\n",
        "param_grid_kmeans = {\n",
        "    'n_clusters': [3],\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'n_init': [10, 20, 30],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'random_state': [0, 42]\n",
        "}\n",
        "\n",
        "param_grid_kmedoids = {\n",
        "    'n_clusters': [3],\n",
        "    'init': ['random'],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'random_state': [0, 42]\n",
        "}\n",
        "\n",
        "# Define GridSearchCV objects\n",
        "grid_search_agglomerative = GridSearchCV(clustering_algorithms['Agglomerative'], param_grid_agglomerative, scoring='adjusted_rand_score', cv=5)\n",
        "grid_search_birch = GridSearchCV(clustering_algorithms['birch'], param_grid_birch, scoring='adjusted_rand_score', cv=5)\n",
        "grid_search_kmeans = GridSearchCV(clustering_algorithms['KMeans'], param_grid_kmeans, scoring='adjusted_rand_score', cv=5)\n",
        "grid_search_kmedoids = GridSearchCV(clustering_algorithms['KMedoids'], param_grid_kmedoids, scoring='adjusted_rand_score', cv=5)\n",
        "\n",
        "results = []\n",
        "\n",
        "# Initialize a dictionary to store the best results\n",
        "best_results = {}\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "        if algorithm_name == 'Agglomerative':\n",
        "            grid_search = grid_search_agglomerative\n",
        "        elif algorithm_name == 'birch':\n",
        "            grid_search = grid_search_birch\n",
        "        elif algorithm_name == 'KMeans':\n",
        "            grid_search = grid_search_kmeans\n",
        "        elif algorithm_name == 'KMedoids':\n",
        "            grid_search = grid_search_kmedoids\n",
        "\n",
        "        # Use GridSearchCV to find the best parameters\n",
        "        grid_search.fit(scaled_data)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "        if algorithm_name == 'Agglomerative':\n",
        "            best_algorithm = AgglomerativeClustering(n_clusters=best_params['n_clusters'],\n",
        "                                                  affinity=best_params['affinity'],\n",
        "                                                  linkage=best_params['linkage'])\n",
        "        elif algorithm_name == 'birch':\n",
        "            best_algorithm = Birch(threshold=best_params['threshold'],\n",
        "                                   branching_factor=best_params['branching_factor'],\n",
        "                                   n_clusters=best_params['n_clusters'],\n",
        "                                   compute_labels=best_params['compute_labels'])\n",
        "        elif algorithm_name == 'KMeans':\n",
        "            best_algorithm = KMeans(n_clusters=best_params['n_clusters'],\n",
        "                                    init=best_params['init'],\n",
        "                                    n_init=best_params['n_init'],\n",
        "                                    max_iter=best_params['max_iter'],\n",
        "                                    random_state=best_params['random_state'])\n",
        "        elif algorithm_name == 'KMedoids':\n",
        "            best_algorithm = KMedoids(n_clusters=best_params['n_clusters'],\n",
        "                                      init=best_params['init'],\n",
        "                                      max_iter=best_params['max_iter'],\n",
        "                                      random_state=best_params['random_state'])\n",
        "\n",
        "        cluster_labels = best_algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            score = adjusted_rand_score(scaled_data.argmax(axis=1), cluster_labels)\n",
        "            results.append({'Metric': 'adjusted_rand_score',\n",
        "                            'Dataset': dataset_label,\n",
        "                            'Method': algorithm_name,\n",
        "                            'Score': score})\n",
        "\n",
        "            # Check if this result is the best so far for this algorithm\n",
        "            if algorithm_name not in best_results or score > best_results[algorithm_name]['Score']:\n",
        "                best_results[algorithm_name] = {'Params': best_params, 'Score': score}\n",
        "\n",
        "# Print and save the best results\n",
        "best_results_df = pd.DataFrame(best_results).transpose()\n",
        "best_results_df.to_csv('best_results.csv')\n",
        "\n",
        "print(\"Best Results:\")\n",
        "print(best_results_df)\n"
      ],
      "metadata": {
        "id": "2vBhGxCvX36t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering, Birch, KMeans\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Define the list of files\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering algorithms\n",
        "clustering_algorithms = {\n",
        "    'Agglomerative': AgglomerativeClustering(),\n",
        "    'birch': Birch(),\n",
        "    'KMeans': KMeans(),\n",
        "    'KMedoids': KMedoids()\n",
        "}\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grid_agglomerative = {\n",
        "    'n_clusters': [3],\n",
        "    'affinity': ['euclidean', 'manhattan'],\n",
        "    'linkage': ['ward', 'complete', 'average']\n",
        "}\n",
        "\n",
        "param_grid_birch = {\n",
        "    'threshold': [0.1, 0.3, 0.5],\n",
        "    'branching_factor': [50, 100, 200],\n",
        "    'n_clusters': [2, 3, 4, 5],\n",
        "    'compute_labels': [True, False]\n",
        "}\n",
        "\n",
        "param_grid_kmeans = {\n",
        "    'n_clusters': [3],\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'n_init': [10, 20, 30],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'random_state': [0, 42]\n",
        "}\n",
        "\n",
        "param_grid_kmedoids = {\n",
        "    'n_clusters': [3],\n",
        "    'init': ['random'],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'random_state': [0, 42]\n",
        "}\n",
        "\n",
        "# Define GridSearchCV objects\n",
        "grid_search_agglomerative = GridSearchCV(clustering_algorithms['Agglomerative'], param_grid_agglomerative, scoring='adjusted_rand_score', cv=5)\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "# Initialize a dictionary to store the best results\n",
        "best_results = {}\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "        if algorithm_name == 'Agglomerative':\n",
        "            grid_search = grid_search_agglomerative\n",
        "\n",
        "\n",
        "        # Use GridSearchCV to find the best parameters\n",
        "        grid_search.fit(scaled_data)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "        if algorithm_name == 'Agglomerative':\n",
        "            best_algorithm = AgglomerativeClustering(n_clusters=best_params['n_clusters'],\n",
        "                                                  affinity=best_params['affinity'],\n",
        "                                                  linkage=best_params['linkage'])\n",
        "\n",
        "\n",
        "        cluster_labels = best_algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            score = adjusted_rand_score(scaled_data.argmax(axis=1), cluster_labels)\n",
        "            results.append({'Metric': 'adjusted_rand_score',\n",
        "                            'Dataset': dataset_label,\n",
        "                            'Method': algorithm_name,\n",
        "                            'Score': score})\n",
        "\n",
        "            # Check if this result is the best so far for this algorithm\n",
        "            if algorithm_name not in best_results or score > best_results[algorithm_name]['Score']:\n",
        "                best_results[algorithm_name] = {'Params': best_params, 'Score': score}\n",
        "\n",
        "# Print and save the best results\n",
        "best_results_df = pd.DataFrame(best_results).transpose()\n",
        "best_results_df.to_csv('best_results.csv')\n",
        "\n",
        "print(\"Best Results:\")\n",
        "print(best_results_df)"
      ],
      "metadata": {
        "id": "rOMRmNJapSgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering algorithms\n",
        "clustering_algorithms = {\n",
        "    'Agglomerative': AgglomerativeClustering()\n",
        "}\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_clusters': [3],  # Example: You can change this list to try different values\n",
        "    'affinity': ['euclidean', 'manhattan'],\n",
        "    'linkage': ['ward', 'complete', 'average']\n",
        "}\n",
        "\n",
        "# Define GridSearchCV object\n",
        "grid_search = GridSearchCV(clustering_algorithms['Agglomerative'], param_grid, scoring='adjusted_rand_score', cv=5)\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "        # Use GridSearchCV to find the best parameters\n",
        "        grid_search.fit(scaled_data)\n",
        "        best_params = grid_search.best_params_\n",
        "        best_algorithm = AgglomerativeClustering(n_clusters=best_params['n_clusters'],\n",
        "                                                  affinity=best_params['affinity'],\n",
        "                                                  linkage=best_params['linkage'])\n",
        "\n",
        "        cluster_labels = best_algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            score = adjusted_rand_score(scaled_data.argmax(axis=1), cluster_labels)\n",
        "            results.append({'Metric': 'adjusted_rand_score',\n",
        "                            'Dataset': dataset_label,\n",
        "                            'Method': algorithm_name,\n",
        "                            'Score': score})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Append the best result to the CSV file with algorithm name\n",
        "best_result = results_df[results_df['Score'] == results_df['Score'].max()]\n",
        "best_result.to_csv('tuning-parameter-agglomerative.csv', mode='a', header=False, index=False)  # Append mode, no header\n",
        "\n",
        "# Print the best result\n",
        "print(\"Best Result:\")\n",
        "print(best_result)\n",
        "\n"
      ],
      "metadata": {
        "id": "MrETg3PbBaDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "bYVjCiK7Vgjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness'] # please exclude those features that you dont need them\n",
        "                                                                                           #First we do the expriments with Hardness alone and then Modulus Alone and then The combination of them\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "              #metric = {'euclidean', 'manhattan', 'chebyshev'}.\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "                     #linkage = {'ward' (default) , 'complete' , 'average' , 'single'}\n",
        "                     #affinity= {'euclidean' , 'manhattan , 'cosine'}\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "                     #metric = {c , 'precomputed'}\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "                    #metric = {'eudliadean' (default) , 'manhattan' , 'cosine' }\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': silhouette_score,\n",
        "    'Calinski-Harabasz': calinski_harabasz_score,\n",
        "    'Davies-Bouldin': davies_bouldin_score\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                score = metric_func(scaled_data, cluster_labels)\n",
        "                results.append({'Metric': metric_name,\n",
        "                                'Dataset': dataset_label,\n",
        "                                'Method': algorithm_name,\n",
        "                                'Score': score})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "grouped = results_df.groupby(['Metric', 'Dataset', 'Method']).Score.mean().unstack()\n",
        "\n",
        "\n",
        "if os.path.exists(\"HMresults.xlsx\"):\n",
        "    os.remove(\"HMresults.xlsx\")\n",
        "\n",
        "grouped.to_excel(\"HMresults.xlsx\")\n",
        "print(grouped.to_latex())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sF2jU4MxBCl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"HMresults.xlsx\", engine='openpyxl')\n",
        "\n",
        "\n",
        "metric_columns = df.columns.difference(['Metric', 'Dataset'])\n",
        "\n",
        "\n",
        "updated_rows = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "\n",
        "    updated_rows = updated_rows.append(row)\n",
        "\n",
        "\n",
        "    if row['Dataset'] == 'D4':\n",
        "\n",
        "        last_4_rows = df.iloc[idx-3:idx+1][metric_columns]\n",
        "        avg_values = last_4_rows.mean()\n",
        "\n",
        "\n",
        "        avg_row_data = row.to_dict()\n",
        "        avg_row_data.update(avg_values)\n",
        "        avg_row_data['Dataset'] = 'Avg'\n",
        "\n",
        "\n",
        "        updated_rows = updated_rows.append(avg_row_data, ignore_index=True)\n",
        "\n",
        "\n",
        "df_updated = updated_rows.reset_index(drop=True)\n",
        "df_updated.to_excel(\"HMresults2.xlsx\", index=False)\n",
        "\n",
        "\n",
        "print(df_updated)"
      ],
      "metadata": {
        "id": "hSPKo5kcGrfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"HMresults2.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "\n",
        "averages = df.groupby(level=0).mean()\n",
        "\n",
        "\n",
        "for metric in averages.index:\n",
        "    df.loc[(metric, 'Avg'), :] = averages.loc[metric]\n",
        "\n",
        "# Determine rankings for the averages. Higher is better for Silhouette and Calinski-Harabasz,\n",
        "# while lower is better for Davies-Bouldin.\n",
        "metrics = df.index.get_level_values(0).unique()\n",
        "for metric in metrics:\n",
        "    if metric in [\"Silhouette\", \"Calinski-Harabasz\"]:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=False).astype(int)\n",
        "    else:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=True).astype(int)\n",
        "    df.loc[(metric, 'Rank'), :] = rank\n",
        "\n",
        "\n",
        "order = ['D1', 'D2', 'D3', 'D4', 'Avg', 'Rank']\n",
        "sorted_tuples = sorted(df.index, key=lambda x: (metrics.tolist().index(x[0]), order.index(x[1])))\n",
        "df = df.reindex(sorted_tuples)\n",
        "\n",
        "\n",
        "df.to_excel(\"HMresults.xlsx\")\n",
        "print(df.to_latex())\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "vUvKPOuYKUO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample Data loading\n",
        "df = pd.read_excel(\"Mresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings = df.xs('Rank', level=1)\n",
        "\n",
        "colors = {\n",
        "    'Silhouette': 'green',\n",
        "    'Davies-Bouldin': 'blue',\n",
        "    'Calinski-Harabasz': 'orange'\n",
        "}\n",
        "\n",
        "algorithms_order = ['Agglomerative', 'BIRCH', 'DBSCAN', 'FCM', 'GMM', 'KMeans', 'KMedoids', 'OPTICS']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(rankings), figsize=(15, 5))\n",
        "\n",
        "\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "for ax, (metric, data) in zip(axes, rankings.iterrows()):\n",
        "    # Re-order the rankings such that 8 is at the bottom and 1 at the top\n",
        "    visual_rank = 9 - data.reindex(algorithms_order)  # 9 - rank to invert the bars\n",
        "    bars = ax.bar(visual_rank.index, visual_rank, color=colors[metric], width=0.5)\n",
        "\n",
        "    highest_rank_bar = data.idxmin()\n",
        "\n",
        "    # Highlight the highest rank bar with a red rectangle\n",
        "    for bar in bars:\n",
        "        if 9 - bar.get_height() == 1:  # find the rank 1 bar\n",
        "            rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "            ax.add_patch(rect)\n",
        "            break\n",
        "\n",
        "    ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim(0, 9)  # Set the y-axis limits\n",
        "    ax.set_yticks(range(1, 9))\n",
        "    ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])  # Explicitly setting the y-tick labels\n",
        "    ax.set_title(metric, fontweight='bold')\n",
        "    ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "# Improve layout for better presentation\n",
        "plt.tight_layout()\n",
        "\n",
        "# High resolution saving for academic papers\n",
        "plt.savefig('Mresults__evauation3.png', dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Di7GXnIJPJ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def kendall_tau_distance(rank_A, rank_B):\n",
        "    \"\"\"Calculate the number of pairwise disagreements between two rankings.\"\"\"\n",
        "    n = len(rank_A)\n",
        "    concordant = 0\n",
        "    discordant = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            a = rank_A[i] - rank_A[j]\n",
        "            b = rank_B[i] - rank_B[j]\n",
        "\n",
        "            if a * b > 0:\n",
        "                concordant += 1\n",
        "            elif a * b < 0:\n",
        "                discordant += 1\n",
        "\n",
        "    return discordant - concordant\n",
        "\n",
        "def kemeny_young_rankings(rankings):\n",
        "    \"\"\"Compute the Kemeny-Young method rankings.\"\"\"\n",
        "    aggregate_rankings = []\n",
        "    all_possible_rankings = list(itertools.permutations(rankings[0]))\n",
        "    for ranking in all_possible_rankings:\n",
        "        distance = sum(kendall_tau_distance(list(ranking), list(rank)) for rank in rankings)\n",
        "        aggregate_rankings.append((ranking, distance))\n",
        "    # The optimal ranking is the one with the smallest total distance\n",
        "    ky_ranking = min(aggregate_rankings, key=lambda x: x[1])[0]\n",
        "    return ky_ranking\n",
        "\n",
        "xls = pd.ExcelFile('aggregate-ranking.xlsx')\n",
        "\n",
        "# Titles mapping\n",
        "title_map = {\n",
        "    \"HM-ranking\": \"(c)Hardness+Modulus\",\n",
        "    \"H-ranking\": \"(a)Hardness\",\n",
        "    \"M-ranking\": \"(b)Modulus\"\n",
        "}\n",
        "\n",
        "# Desired order for plotting\n",
        "sheet_order = [\"H-ranking\", \"M-ranking\", \"HM-ranking\"]\n",
        "\n",
        "# Only consider the sheets in the desired order\n",
        "sheet_names = [sheet for sheet in sheet_order if sheet in xls.sheet_names]\n",
        "\n",
        "# Ordering of the algorithms\n",
        "algorithm_order = [\"Agglomerative\", \"BIRCH\", \"DBSCAN\", \"FCM\", \"GMM\", \"KMeans\", \"KMedoids\", \"OPTICS\"]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(sheet_names), figsize=(15, 5))\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "\n",
        "with pd.ExcelWriter('aggregate-ranking.xlsx', engine='openpyxl', mode='a') as writer:\n",
        "    for ax, sheet_name in zip(axes, sheet_names):\n",
        "        sheet_data = xls.parse(sheet_name)\n",
        "        rankings = sheet_data.iloc[1:].values.tolist()\n",
        "        aggregate_ranking = kemeny_young_rankings(rankings)\n",
        "        print(f'For {sheet_name}, the aggregated rank is {aggregate_ranking}')\n",
        "\n",
        "\n",
        "        aggregate_df = pd.DataFrame({\"Algorithm\": algorithm_order, \"Aggregate Rank\": aggregate_ranking})\n",
        "        aggregate_df.to_excel(writer, sheet_name=f\"{sheet_name}_AggregateRank\", index=False)\n",
        "\n",
        "\n",
        "        visual_data = pd.Series(aggregate_ranking, index=algorithm_order)\n",
        "        visual_rank = 9 - visual_data.reindex(algorithm_order)\n",
        "\n",
        "        bars = ax.bar(visual_rank.index, visual_rank, color='blue', width=0.5)\n",
        "        for bar in bars:\n",
        "            if 9 - bar.get_height() == 1:\n",
        "                rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "                ax.add_patch(rect)\n",
        "                break\n",
        "\n",
        "        ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylim(0, 9)\n",
        "        ax.set_yticks(range(1, 9))\n",
        "        ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])\n",
        "\n",
        "        # Set the title using the title_map dictionary\n",
        "        ax.set_title(title_map.get(sheet_name, sheet_name), fontweight='bold')\n",
        "\n",
        "        ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('aggregate_ranking_evaluation.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "xls = pd.ExcelFile('aggregate-ranking.xlsx')\n",
        "\n",
        "\n",
        "aggregate_sheets = ['H-ranking_AggregateRank', 'M-ranking_AggregateRank', 'HM-ranking_AggregateRank']\n",
        "aggregate_rankings = []\n",
        "\n",
        "for sheet in aggregate_sheets:\n",
        "    data = xls.parse(sheet)\n",
        "    ranking = data['Aggregate Rank'].tolist()\n",
        "    aggregate_rankings.append(ranking)\n",
        "\n",
        "\n",
        "aggregate_sheets = ['H-ranking_AggregateRank', 'M-ranking_AggregateRank', 'HM-ranking_AggregateRank']\n",
        "aggregate_rankings = []\n",
        "\n",
        "for sheet in aggregate_sheets:\n",
        "    data = xls.parse(sheet)\n",
        "    ranking = data['Aggregate Rank'].tolist()\n",
        "    aggregate_rankings.append(ranking)\n",
        "\n",
        "# Find the final order using the Kemeny-Young method\n",
        "final_order = kemeny_young_rankings(aggregate_rankings)\n",
        "print(f'The final aggregated rank is {final_order}')\n",
        "\n",
        "# Visualize the final order\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "visual_data = pd.Series(final_order, index=algorithm_order)\n",
        "visual_rank = 9 - visual_data.reindex(algorithm_order)\n",
        "\n",
        "bars = ax.bar(visual_rank.index, visual_rank, color='blue', width=0.5)\n",
        "for bar in bars:\n",
        "    if 9 - bar.get_height() == 1:\n",
        "        rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "        ax.add_patch(rect)\n",
        "        break\n",
        "\n",
        "ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(0, 9)\n",
        "ax.set_yticks(range(1, 9))\n",
        "ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])\n",
        "ax.set_title('Final Ranking', fontweight='bold')\n",
        "ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('final_ranking_evaluation.png', dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_X4sDIHKeH9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def kendall_tau_distance(rank_A, rank_B):\n",
        "    n = len(rank_A)\n",
        "    pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n",
        "    disagreements = 0\n",
        "    for x, y in pairs:\n",
        "        a = rank_A[x] - rank_A[y]\n",
        "        b = rank_B[x] - rank_B[y]\n",
        "        if a * b < 0:\n",
        "            disagreements += 1\n",
        "    return disagreements\n",
        "\n",
        "def kemeny_young(rankings):\n",
        "    n = len(rankings[0])\n",
        "    min_distance = float('inf')\n",
        "    best_ranking = None\n",
        "    for candidate in permutations(range(n)):\n",
        "        total_distance = sum(kendall_tau_distance(candidate, rank) for rank in rankings)\n",
        "        if total_distance < min_distance:\n",
        "            min_distance = total_distance\n",
        "            best_ranking = candidate\n",
        "    return best_ranking\n",
        "\n",
        "df = pd.read_excel(\"Hresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings_df = df.xs('Rank', level=1)\n",
        "methods_order = [\"KMeans\", \"KMedoids\", \"FCM\", \"Agglomerative\", \"BIRCH\", \"DBSCAN\", \"OPTICS\" , \"GMM\"]\n",
        "rankings = [list(row[method] for method in methods_order) for _, row in rankings_df.iterrows()]\n",
        "aggregate_ranking = [methods_order[i] for i in kemeny_young(rankings)]\n",
        "\n",
        "\n",
        "height_dict = {method: 9 - idx for idx, method in enumerate(aggregate_ranking)}\n",
        "\n",
        "\n",
        "bar_heights = [height_dict[method] for method in methods_order]\n",
        "\n",
        "hardness_results_df = pd.DataFrame({\n",
        "    'Clustering Methods': methods_order,\n",
        "    'Rank': [9 - height_dict[method] for method in methods_order]\n",
        "})\n",
        "\n",
        "\n",
        "hardness_results_df = hardness_results_df.sort_values(by='Rank')\n",
        "\n",
        "\n",
        "hardness_results_df.to_excel(\"hardness_ranking_result.xlsx\", index=False)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(range(len(methods_order)), bar_heights, color='blue')\n",
        "ax.set_xticks(range(len(methods_order)))\n",
        "ax.set_xticklabels(methods_order, rotation=90)\n",
        "\n",
        "max_height_idx = bar_heights.index(max(bar_heights))\n",
        "rect_width = 0.95\n",
        "rect_x_start = max_height_idx - (rect_width / 2)\n",
        "rect = Rectangle((rect_x_start, 0), rect_width, max(bar_heights), linewidth=1.5, edgecolor='r', facecolor='none', linestyle='dotted')\n",
        "ax.add_patch(rect)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Clustering Methods')\n",
        "ax.set_ylabel('Rank')\n",
        "ax.set_title('(a) Hardness')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDfAwBWsWuFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the ranking results for hardness, modulus, and hm\n",
        "hardness_df = pd.read_excel('hardness_rank_results.xlsx')\n",
        "modulus_df = pd.read_excel('modulus_rank_results.xlsx')\n",
        "hm_df = pd.read_excel('hm_rank_results.xlsx')\n",
        "\n",
        "# Transpose each dataframe so that the clustering methods become columns\n",
        "hardness_df = hardness_df.T\n",
        "modulus_df = modulus_df.T\n",
        "hm_df = hm_df.T\n",
        "\n",
        "# Extract the ranks for each attribute into a new dataframe\n",
        "data = {\n",
        "    hardness_df.columns[0]: hardness_df.iloc[0].values,\n",
        "    modulus_df.columns[0]: modulus_df.iloc[0].values,\n",
        "    hm_df.columns[0]: hm_df.iloc[0].values\n",
        "}\n",
        "\n",
        "combined_df = pd.DataFrame(data)\n",
        "\n",
        "# Set the index to the names of the clustering methods\n",
        "combined_df.index = hardness_df.index\n",
        "\n",
        "# Transpose the final dataframe so that clustering methods are column headers\n",
        "combined_df = combined_df.transpose()\n",
        "\n",
        "combined_df.to_excel('ready_for_ranking.xlsx')\n",
        "\n"
      ],
      "metadata": {
        "id": "E9rxMGWz0w-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_values_for_metric(filename, metric_row):\n",
        "    \"\"\"Read the file, and return values for the given metric from the specified row.\"\"\"\n",
        "    data = pd.read_excel(filename)\n",
        "\n",
        "\n",
        "    values = data.iloc[metric_row, 2:10].values\n",
        "    return values\n",
        "\n",
        "files = ['Hresults.xlsx', 'HMresults.xlsx', 'Mresults.xlsx']\n",
        "colors = ['blue', 'red', 'black']\n",
        "label_mapper = {\n",
        "    'Hresults': 'Hardness',\n",
        "    'HMresults': 'Hardness + Modulus',\n",
        "    'Mresults': 'Modulus'\n",
        "}\n",
        "file_labels = [label_mapper[filename.split('.')[0]] for filename in files]\n",
        "\n",
        "\n",
        "data_sample = pd.read_excel(files[0])\n",
        "cluster_names = data_sample.columns[2:10]\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': 16,\n",
        "    'Calinski-Harabasz': 4,\n",
        "    'Davies-Bouldin': 10\n",
        "}\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 6), constrained_layout=True)\n",
        "\n",
        "for ax, (metric_name, metric_row) in zip(axs, metrics.items()):\n",
        "\n",
        "    for filename, color, label in zip(files, colors, file_labels):\n",
        "        values = get_values_for_metric(filename, metric_row)\n",
        "        ax.plot(cluster_names, values, marker='o', color=color, linewidth=2, label=label)\n",
        "\n",
        "    ax.set_title(metric_name)\n",
        "    ax.set_xticklabels(cluster_names, rotation=45, fontsize=10, fontname='sans')\n",
        "    ax.grid(True)\n",
        "\n",
        "\n",
        "handles, labels = axs[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TmxJWFgIw2Gs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "name": "evaluation_modulus4.ipynb",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}