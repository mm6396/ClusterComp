{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mm6396/ClusterComp/blob/main/evaluation_modulus7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHucI84mXoGi"
      },
      "source": [
        "##**Imports Section**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***New Expremiments; ***"
      ],
      "metadata": {
        "id": "0UPGXTKg1FnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#Method for Preprocessing datasetsand replacing nulls :\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def processing_procedure(files):\n",
        "    processed_files = []\n",
        "    for filename in files:\n",
        "\n",
        "        df = pd.read_excel(filename)\n",
        "\n",
        "\n",
        "        df.interpolate(method='nearest', inplace=True, limit_direction='both')\n",
        "\n",
        "\n",
        "        processed_filename = filename.split('.')[0] + '_processed.' + filename.split('.')[1]\n",
        "\n",
        "\n",
        "        if os.path.exists(processed_filename):\n",
        "            print(f\"{processed_filename} already exists. Overwriting...\")\n",
        "\n",
        "        # Save (or overwrite) the processed data into the new spreadsheet.\n",
        "        df.to_excel(processed_filename, index=False)\n",
        "\n",
        "        processed_files.append(processed_filename)\n",
        "\n",
        "    return processed_files\n"
      ],
      "metadata": {
        "id": "U_J6KpxiyLut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#------------GENERAL KEMENY YOUNG METHOD----------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "!pip install cylp\n",
        "#sudo apt-get install coinor-cbc coinor-clp\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import itertools\n",
        "import numpy as np\n",
        "from __future__ import division\n",
        "from timeit import default_timer as time\n",
        "from cylp.cy import CyClpSimplex\n",
        "from cylp.py.pivots import PositiveEdgePivot\n",
        "import importlib\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "#from utils import combs, perms\n",
        "#from fairlearn.reductions import extended_condorcet_simple\n",
        "\n",
        "def extended_condorcet_simple(rankings):\n",
        "\n",
        "    # assumes: cands -> 0,N-1\n",
        "    n = rankings.shape[1]\n",
        "    cands = np.arange(n)\n",
        "    pairs = combs(range(n), 2)\n",
        "\n",
        "    condorcet_rows, condorcet_cols = [], []\n",
        "\n",
        "    for cand, other_cand in pairs:\n",
        "        cand_pos = np.where(rankings == cand)[1]\n",
        "        other_pos = np.where(rankings == other_cand)[1]\n",
        "\n",
        "        if np.all(cand_pos < other_pos):\n",
        "            condorcet_rows.append(cand)\n",
        "            condorcet_cols.append(other_cand)\n",
        "        elif np.all(other_pos < cand_pos):\n",
        "            condorcet_rows.append(other_cand)\n",
        "            condorcet_cols.append(cand)\n",
        "\n",
        "    mat = sp.coo_matrix((np.ones(len(condorcet_rows)), (condorcet_rows, condorcet_cols)))\n",
        "    return mat\n",
        "\n",
        "def combs(a, r):\n",
        "    \"\"\"\n",
        "    Return successive r-length combinations of elements in the array a.\n",
        "    Should produce the same output as array(list(combinations(a, r))), but\n",
        "    faster.\n",
        "    \"\"\"\n",
        "    a = np.asarray(a)\n",
        "    dt = np.dtype([('', a.dtype)]*r)\n",
        "    b = np.fromiter(itertools.combinations(a, r), dt)\n",
        "    b_ = b.view(a.dtype).reshape(-1, r)\n",
        "    return b_\n",
        "\n",
        "def perms(a, r):\n",
        "    \"\"\"\n",
        "    Same as above with permutations\n",
        "    \"\"\"\n",
        "    a = np.asarray(a)\n",
        "    dt = np.dtype([('', a.dtype)]*r)\n",
        "    b = np.fromiter(itertools.permutations(a, r), dt)\n",
        "    b_ = b.view(a.dtype).reshape(-1, r)\n",
        "    return b_\n",
        "\n",
        "class KemenyRanking():\n",
        "    def __init__(self, fp, verbose=True, condorcet_red=True):\n",
        "        self.verbose = verbose\n",
        "        self.condorcet_red = True\n",
        "        self.parse_file(fp)\n",
        "        self.build_Q()\n",
        "        self.solve_ilp()\n",
        "        self.postprocess()\n",
        "        self.print_sol()\n",
        "\n",
        "    def parse_file(self, fp):\n",
        "        \"\"\" Reads and preprocesses input \"\"\"\n",
        "        # TODO add checks\n",
        "        # TODO add specification\n",
        "        if self.verbose:\n",
        "            print('Parse input')\n",
        "\n",
        "        with open(fp) as file:\n",
        "            content = file.readlines()\n",
        "            content = [x.strip() for x in content]                          # remove newlines\n",
        "            content = [x.replace(':', '') for x in content]                 # remove \":\"\n",
        "            content = [np.array(x.split(), dtype=object) for x in content]  # split line into list\n",
        "                                                                            # -> array\n",
        "\n",
        "            raw_arr = np.array(content)\n",
        "            self.voters_raw = raw_arr[:, 0]\n",
        "            self.votes_raw = raw_arr[:, 1:]\n",
        "\n",
        "            # Map to 0, N -> only votes!\n",
        "            self.orig2id = {}\n",
        "            self.id2orig = {}\n",
        "            id_ = 0\n",
        "            for i in np.unique(self.votes_raw):\n",
        "                self.orig2id[i] = id_\n",
        "                self.id2orig[id_] = i\n",
        "                id_ += 1\n",
        "            self.votes_arr = np.vectorize(self.orig2id.get)(self.votes_raw)\n",
        "\n",
        "        if self.verbose:\n",
        "            print('     ... finished')\n",
        "\n",
        "            print('Problem statistics')\n",
        "            print('  {} votes'.format(self.votes_arr.shape[0]))\n",
        "            print('  {} candidates'.format(self.votes_arr.shape[1]))\n",
        "\n",
        "    def build_Q(self):\n",
        "        \"\"\" Creates incidence-matrix: form used in MIP-model \"\"\"\n",
        "        if self.verbose:\n",
        "            print('Build incidence-matrix')\n",
        "\n",
        "        N, n = self.votes_arr.shape                                              # N votes, n cands\n",
        "        self.Q = np.zeros((n,n))\n",
        "        for a,b in itertools.combinations(range(n), 2):\n",
        "            a_pos = np.where(self.votes_arr == a)[1]\n",
        "            b_pos = np.where(self.votes_arr == b)[1]\n",
        "            plus = np.count_nonzero(a_pos < b_pos)\n",
        "            minus = np.count_nonzero(a_pos > b_pos)\n",
        "            self.Q[a,b] = plus\n",
        "            self.Q[b,a] = minus\n",
        "\n",
        "        if self.verbose:\n",
        "            print('     ... finished')\n",
        "\n",
        "    def solve_ilp(self):\n",
        "        \"\"\" Solves problem exactly using MIP/ILP approach\n",
        "            Used solver: CoinOR CBC\n",
        "            Incidence-matrix Q holds complete information needed for opt-process\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print('Solve: build model')\n",
        "\n",
        "        if self.condorcet_red:\n",
        "            condorcet_red_mat = extended_condorcet_simple(self.votes_arr)\n",
        "\n",
        "        n = self.Q.shape[0]\n",
        "        x_n = n*n\n",
        "\n",
        "        model = CyClpSimplex()                                           # MODEL\n",
        "        x = model.addVariable('x', x_n, isInt=True)                      # VARS\n",
        "\n",
        "        model.objective = self.Q.ravel()                                 # OBJ\n",
        "\n",
        "        # x_ab = boolean (already int; need to constrain to [0,1])\n",
        "        model += sp.eye(x_n) * x >= np.zeros(x_n)\n",
        "        model += sp.eye(x_n) * x <= np.ones(x_n)\n",
        "\n",
        "        idx = lambda i, j: np.ravel_multi_index((i, j), (n,n))\n",
        "\n",
        "        # constraints for every pair\n",
        "        start_time = time()\n",
        "        n_pairwise_constr = n*(n-1)//2\n",
        "        if self.verbose:\n",
        "            print('  # pairwise constr: ', n_pairwise_constr)\n",
        "\n",
        "        # Somewhat bloated just to get some vectorization / speed !\n",
        "        combs_ = combs(range(n), 2)\n",
        "\n",
        "        inds_a = np.ravel_multi_index(combs_.T, (n, n))\n",
        "        inds_b = np.ravel_multi_index(combs_.T[::-1], (n, n))\n",
        "\n",
        "        row_inds = np.tile(np.arange(n_pairwise_constr), 2)\n",
        "        col_inds = np.hstack((inds_a, inds_b))\n",
        "\n",
        "        pairwise_constraints = sp.coo_matrix((np.ones(n_pairwise_constr*2),\n",
        "                                              (row_inds, col_inds)),\n",
        "                                              shape=(n_pairwise_constr, n*n))\n",
        "        end_time = time()\n",
        "        if self.verbose:\n",
        "            print(\"    Took {:.{prec}f} secs\".format(end_time - start_time, prec=3))\n",
        "\n",
        "        # and for every cycle of length 3\n",
        "        start_time = time()\n",
        "        n_triangle_constrs = n*(n-1)*(n-2)\n",
        "        if self.verbose:\n",
        "            print('  # triangle constr: ', n_triangle_constrs)\n",
        "\n",
        "        # Somewhat bloated just to get some vectorization / speed !\n",
        "        perms_ = perms(range(n), 3)\n",
        "\n",
        "        inds_a = np.ravel_multi_index(perms_.T[(0,1), :], (n, n))\n",
        "        inds_b = np.ravel_multi_index(perms_.T[(1,2), :], (n, n))\n",
        "        inds_c = np.ravel_multi_index(perms_.T[(2,0), :], (n, n))\n",
        "\n",
        "        row_inds = np.tile(np.arange(n_triangle_constrs), 3)\n",
        "        col_inds = np.hstack((inds_a, inds_b, inds_c))\n",
        "\n",
        "        triangle_constraints = sp.coo_matrix((np.ones(n_triangle_constrs*3),\n",
        "                                              (row_inds, col_inds)),\n",
        "                                              shape=(n_triangle_constrs, n*n))\n",
        "        end_time = time()\n",
        "        if self.verbose:\n",
        "            print(\"    Took {:.{prec}f} secs\".format(end_time - start_time, prec=3))\n",
        "\n",
        "\n",
        "        model += pairwise_constraints * x == np.ones(n_pairwise_constr)\n",
        "        model += triangle_constraints * x >= np.ones(n_triangle_constrs)\n",
        "\n",
        "        if self.condorcet_red:\n",
        "            I, J, V = sp.find(condorcet_red_mat)\n",
        "            indices_pos = np.ravel_multi_index([J, I], (n,n))\n",
        "            indices_neg = np.ravel_multi_index([I, J], (n,n))\n",
        "            nnz = len(indices_pos)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('  Extended Condorcet reductions: {} * 2 relations fixed'.format(nnz))\n",
        "\n",
        "            lhs = sp.coo_matrix((np.ones(nnz*2),\n",
        "                        (np.arange(nnz*2),\n",
        "                         np.hstack((indices_pos, indices_neg)))),\n",
        "                  shape=(nnz*2, n*n))\n",
        "            rhs = np.hstack((np.ones(len(indices_pos)), np.zeros(len(indices_neg))))\n",
        "            model += lhs * x == rhs\n",
        "\n",
        "        cbcModel = model.getCbcModel()  # Clp -> Cbc model / LP -> MIP\n",
        "        cbcModel.logLevel = self.verbose\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Solve: run MIP\\n')\n",
        "        start_time = time()\n",
        "        status = cbcModel.solve()           #-> \"Call CbcMain. Solve the problem\n",
        "                                            #   \"using the same parameters used\n",
        "                                            #   \"by CbcSolver.\"\n",
        "                                            # This deviates from cylp's docs which are sparse!\n",
        "                                            # -> preprocessing will be used and is very important!\n",
        "        end_time = time()\n",
        "        if self.verbose:\n",
        "            print(\"  CoinOR CBC used {:.{prec}f} secs\".format(end_time - start_time, prec=3))\n",
        "\n",
        "        x_sol = cbcModel.primalVariableSolution['x']\n",
        "        self.obj_sol = cbcModel.objectiveValue\n",
        "        x = np.array(x_sol).reshape((n, n)).round().astype(int)\n",
        "        self.aggr_rank = np.argsort(x.sum(axis=0))[::-1]\n",
        "\n",
        "    def postprocess(self):\n",
        "        if self.verbose:\n",
        "            print('Postprocessing')\n",
        "        self.final_solution = np.vectorize(self.id2orig.get)(self.aggr_rank)\n",
        "        if self.verbose:\n",
        "            print('    ... finished')\n",
        "\n",
        "    def print_sol(self):\n",
        "        print('--------')\n",
        "        print('SOLUTION')\n",
        "        print('  objective: ', self.obj_sol)\n",
        "        print('  aggregation: ')\n",
        "        print(self.final_solution)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Please Run : KemenyRanking('/content/yourfile_name.txt')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lXdyYu9CLAPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulizing Data\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop('Depth', axis=1) for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering methods\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3 , linkage = 'complete'), #linkage = {'ward' , 'complete' , 'average' , 'single'}\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "# Visualization\n",
        "n_rows = len(scaled_datasets)\n",
        "n_cols = len(clustering_algorithms)\n",
        "\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 15))\n",
        "\n",
        "for row, scaled_data in enumerate(scaled_datasets):\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for col, (algorithm_name, algorithm) in enumerate(clustering_algorithms.items()):\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        axs[row, col].scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        axs[row, col].set_title(f'Dataset {row + 1} using {algorithm_name}')\n",
        "\n",
        "\n",
        "        if row == 0:\n",
        "            axs[row, col].set_xlabel(algorithm_name)\n",
        "        if col == 0:\n",
        "            axs[row, col].set_ylabel(f'Dataset {row + 1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7BGrLWvwdHkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------------\n",
        "#Visulization Stifness based on X and Y positions\n",
        "#------------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Stiffness'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Stiffness')\n",
        "plt.suptitle('Stiffness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "# visulization Load based on x and Y positions\n",
        "#------------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Load'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Load')\n",
        "\n",
        "plt.suptitle('Load based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#Vitualization MODULUS based X and Y positions\n",
        "#-------------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['MODULUS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Modulus')\n",
        "\n",
        "plt.suptitle('Modulus based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#Show HARDNESS based on X and Y positions\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['HARDNESS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Hardness')\n",
        "\n",
        "plt.suptitle('Hardness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jA0by-6JbK3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intall the required library packages\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means"
      ],
      "metadata": {
        "id": "yRxJH-qcoQTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for Agglomerative\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import os,csv\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "param_grid = {\n",
        "    'n_clusters': [3],\n",
        "    'affinity': ['euclidean', 'manhattan'],\n",
        "    'linkage': ['ward', 'complete', 'average']\n",
        "}\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "# Mapping of files to the position we're interested in\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "    # tuning\n",
        "    position = position_mapping[idx]\n",
        "    tuning_data = df[df[position] > 125]\n",
        "\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "\n",
        "\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for n_clusters in param_grid['n_clusters']:\n",
        "        for affinity in param_grid['affinity']:\n",
        "            for linkage in param_grid['linkage']:\n",
        "                # 'ward' can only work with 'euclidean'\n",
        "                if linkage == 'ward' and affinity != 'euclidean':\n",
        "                    continue\n",
        "\n",
        "                model = AgglomerativeClustering(n_clusters=n_clusters, affinity=affinity, linkage=linkage)\n",
        "                labels = model.fit_predict(scaled_eval)\n",
        "\n",
        "                score = silhouette_score(scaled_eval, labels)\n",
        "\n",
        "                # Append results to the list\n",
        "                results.append({\n",
        "                    'file': eval_filename,\n",
        "                    'n_clusters': n_clusters,\n",
        "                    'affinity': affinity,\n",
        "                    'linkage': linkage,\n",
        "                    'silhouette_score': score\n",
        "                })\n",
        "\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:4].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:4], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"Agglomerative+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_kemeny_agglomerative.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MrETg3PbBaDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KemenyRanking('/content/tuning_list_kemeny_agglomerative.txt')"
      ],
      "metadata": {
        "id": "qjzcRfU53y6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qlu7d4DeK-P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for KMeans\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "param_grid_KMeans = {\n",
        "    'n_clusters': [3],\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'n_init': [10, 20, 30],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'random_state': [0, 42]\n",
        "}\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    tuning_data = df[df[position] > 125]\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for n_clusters in param_grid_KMeans['n_clusters']:\n",
        "        for init in param_grid_KMeans['init']:\n",
        "            for n_init in param_grid_KMeans['n_init']:\n",
        "                for max_iter in param_grid_KMeans['max_iter']:\n",
        "                    for random_state in param_grid_KMeans['random_state']:\n",
        "                        # KMeans clustering\n",
        "                        model = KMeans(n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter, random_state=random_state)\n",
        "                        labels = model.fit_predict(scaled_eval)\n",
        "\n",
        "                        score = silhouette_score(scaled_eval, labels)\n",
        "\n",
        "                        # Append results to the list\n",
        "                        results.append({\n",
        "                            'file': eval_filename,\n",
        "                            'n_clusters': n_clusters,\n",
        "                            'init': init,\n",
        "                            'n_init': n_init,\n",
        "                            'max_iter': max_iter,\n",
        "                            'random_state': random_state,\n",
        "                            'silhouette_score': score\n",
        "                        })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:6].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:6], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"Kmeans+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_Kemeny_kmeans.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n"
      ],
      "metadata": {
        "id": "MmaorLCahe1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KemenyRanking('/content/tuning_list_Kemeny_kmeans.txt')"
      ],
      "metadata": {
        "id": "Brz_IVZ56tNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for DBSCAN\n",
        "#-------------------------------------------------------------------------------\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN  # Import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "param_grid_DBSCAN = {\n",
        "    'eps': [0.1, 0.2, 0.3],\n",
        "    'min_samples': [5, 10, 15]\n",
        "}\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    tuning_data = df[df[position] > 125]\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for eps in param_grid_DBSCAN['eps']:\n",
        "        for min_samples in param_grid_DBSCAN['min_samples']:\n",
        "            # DBSCAN clustering\n",
        "            model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = model.fit_predict(scaled_eval)\n",
        "\n",
        "            # DBSCAN can produce -1 labels for noise, so we need to filter them out for silhouette score\n",
        "            core_samples_mask = labels != -1\n",
        "            labels_core = labels[core_samples_mask]\n",
        "            scaled_eval_core = scaled_eval[core_samples_mask]\n",
        "\n",
        "            # Silhouette score is only meaningful if there's more than one cluster found\n",
        "            if len(set(labels_core)) > 1:\n",
        "                score = silhouette_score(scaled_eval_core, labels_core)\n",
        "            else:\n",
        "                score = -1\n",
        "\n",
        "            # Append results to the list\n",
        "            results.append({\n",
        "                'file': eval_filename,\n",
        "                'eps': eps,\n",
        "                'min_samples': min_samples,\n",
        "                'silhouette_score': score\n",
        "            })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:3].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:3], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"DBSCAN+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_Kemeny_DBSCAN.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "pVVsTZ_Dojrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KemenyRanking('/content/tuning_list_Kemeny_DBSCAN.txt')"
      ],
      "metadata": {
        "id": "wtGYgbym7Trm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for FCM\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "!pip install Fuzzy-c-means\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from fcmeans import FCM\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "# Columns to be excluded\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    tuning_data = df[df[position] > 125]\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "# Define hyperparameters to search\n",
        "n_clusters_values = [3]\n",
        "m_values = [1.1, 1.5, 2.0]\n",
        "\n",
        "results = []\n",
        "best_score = -1\n",
        "best_params = None\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    scaled_data = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for n_clusters in n_clusters_values:\n",
        "        for m in m_values:\n",
        "            fcm = FCM(n_clusters=n_clusters, m=m)\n",
        "            fcm.fit(scaled_data)\n",
        "            cluster_labels = fcm.predict(scaled_data)\n",
        "\n",
        "            score = silhouette_score(scaled_data, cluster_labels)\n",
        "\n",
        "            results.append({\n",
        "                'file': eval_filename,\n",
        "                'n_clusters': n_clusters,\n",
        "                'm': m,\n",
        "                'silhouette_score': score\n",
        "            })\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:3].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:3], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"FCM+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_Kemeny_FCM.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n"
      ],
      "metadata": {
        "id": "xLFXntiPq9E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KemenyRanking('/content/tuning_list_Kemeny_FCM.txt')"
      ],
      "metadata": {
        "id": "xhoRisnw_2BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for GMM\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "# Columns to be excluded\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    tuning_data = df[df[position] > 125]\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "# GMM parameters\n",
        "param_grid_GMM = {\n",
        "    'n_components': [3],\n",
        "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'random_state': [0, 42]\n",
        "}\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for n_components in param_grid_GMM['n_components']:\n",
        "        for covariance_type in param_grid_GMM['covariance_type']:\n",
        "            for max_iter in param_grid_GMM['max_iter']:\n",
        "                for random_state in param_grid_GMM['random_state']:\n",
        "                    # GMM clustering\n",
        "                    model = GaussianMixture(n_components=n_components, covariance_type=covariance_type, max_iter=max_iter, random_state=random_state)\n",
        "                    labels = model.fit_predict(scaled_eval)\n",
        "\n",
        "                    score = silhouette_score(scaled_eval, labels)\n",
        "\n",
        "                    # Append results to the list\n",
        "                    results.append({\n",
        "                        'file': eval_filename,\n",
        "                        'n_components': n_components,\n",
        "                        'covariance_type': covariance_type,\n",
        "                        'max_iter': max_iter,\n",
        "                        'random_state': random_state,\n",
        "                        'silhouette_score': score\n",
        "                    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:5].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:5], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"GMM+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_Kemeny_GMM.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n"
      ],
      "metadata": {
        "id": "us4JqLFWvYlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KemenyRanking('/content/tuning_list_Kemeny_GMM.txt')"
      ],
      "metadata": {
        "id": "0Tb1P3PgAO0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for Kmedoids\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "# Columns to be excluded\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness','label']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    tuning_data = df[df[position] > 125]\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "# KMedoids parameters\n",
        "param_grid_KMedoids = {\n",
        "    'n_clusters': [3],\n",
        "    'init': ['random'],\n",
        "    'max_iter': [100, 200],\n",
        "    'random_state': [0, 42 ]\n",
        "}\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for n_clusters in param_grid_KMedoids['n_clusters']:\n",
        "        for init in param_grid_KMedoids['init']:\n",
        "            for max_iter in param_grid_KMedoids['max_iter']:\n",
        "                for random_state in param_grid_KMedoids['random_state']:\n",
        "                    # KMedoids clustering\n",
        "                    model = KMedoids(n_clusters=n_clusters, init=init, max_iter=max_iter, random_state=random_state)\n",
        "                    model.fit(scaled_eval)\n",
        "                    labels = model.labels_\n",
        "\n",
        "                    score = silhouette_score(scaled_eval, labels)\n",
        "\n",
        "                    # Append results to the list\n",
        "                    results.append({\n",
        "                        'file': eval_filename,\n",
        "                        'n_clusters': n_clusters,\n",
        "                        'init': init,\n",
        "                        'max_iter': max_iter,\n",
        "                        'random_state': random_state,\n",
        "                        'silhouette_score': score\n",
        "                    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:5].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:5], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"Kmedoids+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_Kemeny_Kmedoids.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "PajPTZzfyBIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KemenyRanking('/content/tuning_list_Kemeny_Kmedoids.txt')"
      ],
      "metadata": {
        "id": "nbhYmVdDLRlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for OPTICS\n",
        "#-------------------------------------------------------------------------------\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import OPTICS\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "# Columns to be excluded\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] > 125]\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "# OPTICS parameters\n",
        "param_grid_OPTICS = {\n",
        "    'min_samples': [3 ,5, 10, 15],\n",
        "    'xi': [0.01,0.03,0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for min_samples in param_grid_OPTICS['min_samples']:\n",
        "        for xi in param_grid_OPTICS['xi']:\n",
        "            # OPTICS clustering\n",
        "            model = OPTICS(min_samples=min_samples, xi=xi)\n",
        "            model.fit(scaled_eval)\n",
        "\n",
        "\n",
        "            if len(set(model.labels_)) <= 1 or (len(set(model.labels_)) == 2 and -1 in model.labels_):\n",
        "               continue\n",
        "\n",
        "            labels = model.labels_\n",
        "            score = silhouette_score(scaled_eval, labels)\n",
        "\n",
        "            # Append results to the list\n",
        "            results.append({\n",
        "                'file': eval_filename,\n",
        "                'min_samples': min_samples,\n",
        "                'xi': xi,\n",
        "                'silhouette_score': score\n",
        "            })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:3].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:3], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"OPTICS+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_Kemeny_OPTICS.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n"
      ],
      "metadata": {
        "id": "Rk0Qbsk20g8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Tuning parameters for BIRCH\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import Birch\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness']\n",
        "\n",
        "eval_files = []\n",
        "tuning_files = []\n",
        "\n",
        "\n",
        "position_mapping = {\n",
        "    0: 'X Position',  # test1\n",
        "    1: 'Y Position',  # test2\n",
        "    2: 'X Position',  # test3\n",
        "    3: 'Y Position'   # test4\n",
        "}\n",
        "\n",
        "for idx, file in enumerate(files_p):\n",
        "    df = pd.read_excel(file, engine='openpyxl')\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] < 125]\n",
        "\n",
        "    eval_filename = f'test_{idx+1}_eval.xlsx'\n",
        "    eval_data.to_excel(eval_filename, index=False)\n",
        "    eval_files.append(eval_filename)\n",
        "\n",
        "\n",
        "\n",
        "    position = position_mapping[idx]\n",
        "    eval_data = df[df[position] > 125]\n",
        "\n",
        "    tuning_filename = f'test_{idx+1}_tuning.xlsx'\n",
        "    tuning_data.to_excel(tuning_filename, index=False)\n",
        "    tuning_files.append(tuning_filename)\n",
        "\n",
        "# BIRCH parameters\n",
        "param_grid_BIRCH = {\n",
        "    'n_clusters': [3],\n",
        "    'threshold': [0.1, 0.2, 0.3],\n",
        "    'branching_factor': [50, 100, 200]\n",
        "}\n",
        "\n",
        "results = []\n",
        "max_silhouette = -1\n",
        "best_params = {}\n",
        "\n",
        "for eval_filename in eval_files:\n",
        "    eval_df = pd.read_excel(eval_filename, engine='openpyxl')\n",
        "    eval_df = eval_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    scaled_eval = StandardScaler().fit_transform(eval_df)\n",
        "\n",
        "    for n_clusters in param_grid_BIRCH['n_clusters']:\n",
        "        for threshold in param_grid_BIRCH['threshold']:\n",
        "            for branching_factor in param_grid_BIRCH['branching_factor']:\n",
        "                # BIRCH clustering\n",
        "                model = Birch(n_clusters=n_clusters, threshold=threshold, branching_factor=branching_factor)\n",
        "                model.fit(scaled_eval)\n",
        "                labels = model.labels_\n",
        "\n",
        "                score = silhouette_score(scaled_eval, labels)\n",
        "\n",
        "                # Append results to the list\n",
        "                results.append({\n",
        "                    'file': eval_filename,\n",
        "                    'n_clusters': n_clusters,\n",
        "                    'threshold': threshold,\n",
        "                    'branching_factor': branching_factor,\n",
        "                    'silhouette_score': score\n",
        "                })\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "def concat_parameters(group):\n",
        "    # This will force even single items into a comma-separated string format\n",
        "\n",
        "    return ' '.join(group['parameters_combinations'].tolist())\n",
        "\n",
        "df_results['parameters_combinations'] = df_results.iloc[:, 1:3].apply(lambda row: '\"' + ','.join(row.dropna().astype(str)) + '\"', axis=1)\n",
        "df_results = df_results.drop(df_results.columns[1:3], axis=1)\n",
        "df_results = df_results.sort_values(by='silhouette_score', ascending=False)\n",
        "df_results['rank'] = df_results.groupby('file')['silhouette_score'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "grouped_combinations = df_results.groupby(['file']).apply(concat_parameters)\n",
        "\n",
        "output_file = \"BIRCH+parameterTuning.xlsx\"\n",
        "df_results.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "with open('tuning_list_Kemeny_BIRCH.txt', 'w') as f:\n",
        "    i = 1\n",
        "    for _, group_string in grouped_combinations.items():\n",
        "        f.write(f'A{i} : {group_string}\\n')\n",
        "        i += 1\n"
      ],
      "metadata": {
        "id": "H7rg5hNIFrFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the XLSX file\n",
        "df = pd.read_excel('parameter-tuning.xlsx')\n",
        "\n",
        "# Step 2: Convert DataFrame to LaTeX\n",
        "latex_code = df.to_latex(index=False)\n",
        "\n",
        "\n",
        "with open('output.tex', 'w') as f:\n",
        "    f.write(latex_code)\n",
        "\n",
        "print('output.tex')"
      ],
      "metadata": {
        "id": "SS0_RokrgFsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating HMresults * in 4 steps"
      ],
      "metadata": {
        "id": "TskUEbvS4VKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating HM-results (1) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness','MODULUS'] # please exclude those features that you dont need them\n",
        "                                                                                           #First we do the expriments with Hardness alone and then Modulus Alone and then The combination of them\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=0 , init= 'k-means++' , n_init = 10 , max_iter = 100 ),\n",
        "\n",
        "    'DBSCAN': DBSCAN(eps=0.1 , min_samples = 5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3 , affinity = 'euclidean' ,  linkage = 'ward'),\n",
        "                     #linkage = {'ward' (default) , 'complete' , 'average' , 'single'}\n",
        "                     #affinity= {'euclidean' , 'manhattan , 'cosine'}\n",
        "    'OPTICS': OPTICS(min_samples = 5 , xi = 0.05),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state= 0 , init = 'random' , max_iter = 100),\n",
        "                     #metric = {c , 'precomputed'}\n",
        "    'GMM': GaussianMixture(n_components=3, max_iter=100 , covariance_type = 'full'),\n",
        "    'BIRCH': Birch(n_clusters=3 , branching_factor = 50 , threshold =0.1),\n",
        "\n",
        "    'FCM': FCM(n_clusters=3 , m = 2)\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': silhouette_score,\n",
        "    'Calinski-Harabasz': calinski_harabasz_score,\n",
        "    'Davies-Bouldin': davies_bouldin_score\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                score = metric_func(scaled_data, cluster_labels)\n",
        "                results.append({'Metric': metric_name,\n",
        "                                'Dataset': dataset_label,\n",
        "                                'Method': algorithm_name,\n",
        "                                'Score': score})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "grouped = results_df.groupby(['Metric', 'Dataset', 'Method']).Score.mean().unstack()\n",
        "\n",
        "\n",
        "if os.path.exists(\"HMresults.xlsx\"):\n",
        "    os.remove(\"HMresults.xlsx\")\n",
        "\n",
        "grouped.to_excel(\"HMresults.xlsx\")\n",
        "print(grouped.to_latex())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sF2jU4MxBCl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating HM-results (2) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"HMresults.xlsx\", engine='openpyxl')\n",
        "\n",
        "\n",
        "metric_columns = df.columns.difference(['Metric', 'Dataset'])\n",
        "\n",
        "\n",
        "updated_rows = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "\n",
        "    updated_rows = updated_rows.append(row)\n",
        "\n",
        "\n",
        "    if row['Dataset'] == 'D4':\n",
        "\n",
        "        last_4_rows = df.iloc[idx-3:idx+1][metric_columns]\n",
        "        avg_values = last_4_rows.mean()\n",
        "\n",
        "\n",
        "        avg_row_data = row.to_dict()\n",
        "        avg_row_data.update(avg_values)\n",
        "        avg_row_data['Dataset'] = 'Avg'\n",
        "\n",
        "\n",
        "        updated_rows = updated_rows.append(avg_row_data, ignore_index=True)\n",
        "\n",
        "\n",
        "df_updated = updated_rows.reset_index(drop=True)\n",
        "df_updated.to_excel(\"HMresults2.xlsx\", index=False)\n",
        "\n",
        "\n",
        "print(df_updated)"
      ],
      "metadata": {
        "id": "hSPKo5kcGrfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# ** generating HM-results (3) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"HMresults2.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "\n",
        "averages = df.groupby(level=0).mean()\n",
        "\n",
        "\n",
        "for metric in averages.index:\n",
        "    df.loc[(metric, 'Avg'), :] = averages.loc[metric]\n",
        "\n",
        "# Determine rankings for the averages. Higher is better for Silhouette and Calinski-Harabasz,\n",
        "# while lower is better for Davies-Bouldin.\n",
        "metrics = df.index.get_level_values(0).unique()\n",
        "for metric in metrics:\n",
        "    if metric in [\"Silhouette\", \"Calinski-Harabasz\"]:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=False).astype(int)\n",
        "    else:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=True).astype(int)\n",
        "    df.loc[(metric, 'Rank'), :] = rank\n",
        "\n",
        "\n",
        "order = ['D1', 'D2', 'D3', 'D4', 'Avg', 'Rank']\n",
        "sorted_tuples = sorted(df.index, key=lambda x: (metrics.tolist().index(x[0]), order.index(x[1])))\n",
        "df = df.reindex(sorted_tuples)\n",
        "\n",
        "\n",
        "df.to_excel(\"HMresults.xlsx\")\n",
        "print(df.to_latex())\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "vUvKPOuYKUO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating HM-results (4) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample Data loading\n",
        "df = pd.read_excel(\"HMresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings = df.xs('Rank', level=1)\n",
        "\n",
        "colors = {\n",
        "    'Silhouette': 'green',\n",
        "    'Davies-Bouldin': 'blue',\n",
        "    'Calinski-Harabasz': 'orange'\n",
        "}\n",
        "\n",
        "algorithms_order = ['Agglomerative', 'BIRCH', 'DBSCAN', 'FCM', 'GMM', 'KMeans', 'KMedoids', 'OPTICS']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(rankings), figsize=(15, 5))\n",
        "\n",
        "\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "for ax, (metric, data) in zip(axes, rankings.iterrows()):\n",
        "    # Re-order the rankings such that 8 is at the bottom and 1 at the top\n",
        "    visual_rank = 9 - data.reindex(algorithms_order)  # 9 - rank to invert the bars\n",
        "    bars = ax.bar(visual_rank.index, visual_rank, color=colors[metric], width=0.5)\n",
        "\n",
        "    highest_rank_bar = data.idxmin()\n",
        "\n",
        "    # Highlight the highest rank bar with a red rectangle\n",
        "    for bar in bars:\n",
        "        if 9 - bar.get_height() == 1:  # find the rank 1 bar\n",
        "            rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "            ax.add_patch(rect)\n",
        "            break\n",
        "\n",
        "    ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim(0, 9)  # Set the y-axis limits\n",
        "    ax.set_yticks(range(1, 9))\n",
        "    ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])  # Explicitly setting the y-tick labels\n",
        "    ax.set_title(metric, fontweight='bold')\n",
        "    ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "# Improve layout for better presentation\n",
        "plt.tight_layout()\n",
        "\n",
        "# High resolution saving for academic papers\n",
        "plt.savefig('HMresults__evauation3.png', dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Di7GXnIJPJ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hresuts.xlsx (in 4 steps)***"
      ],
      "metadata": {
        "id": "B9VY5hBH7htV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating H-results (1) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness', 'MODULUS'] # please exclude those features that you dont need them\n",
        "                                                                                           #First we do the expriments with Hardness alone and then Modulus Alone and then The combination of them\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=0 , init= 'k-means++' , n_init = 10 , max_iter = 100 ),\n",
        "\n",
        "    'DBSCAN': DBSCAN(eps=0.1 , min_samples = 5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3 , affinity = 'euclidean' ,  linkage = 'ward'),\n",
        "                     #linkage = {'ward' (default) , 'complete' , 'average' , 'single'}\n",
        "                     #affinity= {'euclidean' , 'manhattan , 'cosine'}\n",
        "    'OPTICS': OPTICS(min_samples = 5 , xi = 0.05),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state= 0 , init = 'random' , max_iter = 100),\n",
        "                     #metric = {c , 'precomputed'}\n",
        "    'GMM': GaussianMixture(n_components=3, max_iter=100 , covariance_type = 'full'),\n",
        "    'BIRCH': Birch(n_clusters=3 , branching_factor = 50 , threshold =0.1),\n",
        "\n",
        "    'FCM': FCM(n_clusters=3 , m = 2)\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': silhouette_score,\n",
        "    'Calinski-Harabasz': calinski_harabasz_score,\n",
        "    'Davies-Bouldin': davies_bouldin_score\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                score = metric_func(scaled_data, cluster_labels)\n",
        "                results.append({'Metric': metric_name,\n",
        "                                'Dataset': dataset_label,\n",
        "                                'Method': algorithm_name,\n",
        "                                'Score': score})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "grouped = results_df.groupby(['Metric', 'Dataset', 'Method']).Score.mean().unstack()\n",
        "\n",
        "\n",
        "if os.path.exists(\"Hresults.xlsx\"):\n",
        "    os.remove(\"Hresults.xlsx\")\n",
        "\n",
        "grouped.to_excel(\"Hresults.xlsx\")\n",
        "print(grouped.to_latex())\n"
      ],
      "metadata": {
        "id": "5s3LQx_L45oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# ** generating H-results (2) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"Hresults.xlsx\", engine='openpyxl')\n",
        "\n",
        "\n",
        "metric_columns = df.columns.difference(['Metric', 'Dataset'])\n",
        "\n",
        "\n",
        "updated_rows = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "\n",
        "    updated_rows = updated_rows.append(row)\n",
        "\n",
        "\n",
        "    if row['Dataset'] == 'D4':\n",
        "\n",
        "        last_4_rows = df.iloc[idx-3:idx+1][metric_columns]\n",
        "        avg_values = last_4_rows.mean()\n",
        "\n",
        "\n",
        "        avg_row_data = row.to_dict()\n",
        "        avg_row_data.update(avg_values)\n",
        "        avg_row_data['Dataset'] = 'Avg'\n",
        "\n",
        "\n",
        "        updated_rows = updated_rows.append(avg_row_data, ignore_index=True)\n",
        "\n",
        "\n",
        "df_updated = updated_rows.reset_index(drop=True)\n",
        "df_updated.to_excel(\"Hresults2.xlsx\", index=False)\n",
        "\n",
        "\n",
        "print(df_updated)"
      ],
      "metadata": {
        "id": "FlcfkeTA5W5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# ** generating H-results (3) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"Hresults2.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "\n",
        "averages = df.groupby(level=0).mean()\n",
        "\n",
        "\n",
        "for metric in averages.index:\n",
        "    df.loc[(metric, 'Avg'), :] = averages.loc[metric]\n",
        "\n",
        "# Determine rankings for the averages. Higher is better for Silhouette and Calinski-Harabasz,\n",
        "# while lower is better for Davies-Bouldin.\n",
        "metrics = df.index.get_level_values(0).unique()\n",
        "for metric in metrics:\n",
        "    if metric in [\"Silhouette\", \"Calinski-Harabasz\"]:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=False).astype(int)\n",
        "    else:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=True).astype(int)\n",
        "    df.loc[(metric, 'Rank'), :] = rank\n",
        "\n",
        "\n",
        "order = ['D1', 'D2', 'D3', 'D4', 'Avg', 'Rank']\n",
        "sorted_tuples = sorted(df.index, key=lambda x: (metrics.tolist().index(x[0]), order.index(x[1])))\n",
        "df = df.reindex(sorted_tuples)\n",
        "\n",
        "\n",
        "df.to_excel(\"Hresults.xlsx\")\n",
        "print(df.to_latex())\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "doqu_ch25okN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# ** generating H-results (4) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample Data loading\n",
        "df = pd.read_excel(\"Hresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings = df.xs('Rank', level=1)\n",
        "\n",
        "colors = {\n",
        "    'Silhouette': 'green',\n",
        "    'Davies-Bouldin': 'blue',\n",
        "    'Calinski-Harabasz': 'orange'\n",
        "}\n",
        "\n",
        "algorithms_order = ['Agglomerative', 'BIRCH', 'DBSCAN', 'FCM', 'GMM', 'KMeans', 'KMedoids', 'OPTICS']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(rankings), figsize=(15, 5))\n",
        "\n",
        "\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "for ax, (metric, data) in zip(axes, rankings.iterrows()):\n",
        "    # Re-order the rankings such that 8 is at the bottom and 1 at the top\n",
        "    visual_rank = 9 - data.reindex(algorithms_order)  # 9 - rank to invert the bars\n",
        "    bars = ax.bar(visual_rank.index, visual_rank, color=colors[metric], width=0.5)\n",
        "\n",
        "    highest_rank_bar = data.idxmin()\n",
        "\n",
        "    # Highlight the highest rank bar with a red rectangle\n",
        "    for bar in bars:\n",
        "        if 9 - bar.get_height() == 1:  # find the rank 1 bar\n",
        "            rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "            ax.add_patch(rect)\n",
        "            break\n",
        "\n",
        "    ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim(0, 9)  # Set the y-axis limits\n",
        "    ax.set_yticks(range(1, 9))\n",
        "    ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])  # Explicitly setting the y-tick labels\n",
        "    ax.set_title(metric, fontweight='bold')\n",
        "    ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "plt.savefig('Hresults__evauation3.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6qucFdn_5wUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mresults.xlsx (in 4 steps)"
      ],
      "metadata": {
        "id": "ZCLGZI407VAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating M-results (1) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness', 'HARDNESS'] # please exclude those features that you dont need them\n",
        "                                                                                           #First we do the expriments with Hardness alone and then Modulus Alone and then The combination of them\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=0 , init= 'k-means++' , n_init = 10 , max_iter = 100 ),\n",
        "\n",
        "    'DBSCAN': DBSCAN(eps=0.1 , min_samples = 5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3 , affinity = 'euclidean' ,  linkage = 'ward'),\n",
        "                     #linkage = {'ward' (default) , 'complete' , 'average' , 'single'}\n",
        "                     #affinity= {'euclidean' , 'manhattan , 'cosine'}\n",
        "    'OPTICS': OPTICS(min_samples = 5 , xi = 0.05),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state= 0 , init = 'random' , max_iter = 100),\n",
        "                     #metric = {c , 'precomputed'}\n",
        "    'GMM': GaussianMixture(n_components=3, max_iter=100 , covariance_type = 'full'),\n",
        "    'BIRCH': Birch(n_clusters=3 , branching_factor = 50 , threshold =0.1),\n",
        "\n",
        "    'FCM': FCM(n_clusters=3 , m = 2)\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': silhouette_score,\n",
        "    'Calinski-Harabasz': calinski_harabasz_score,\n",
        "    'Davies-Bouldin': davies_bouldin_score\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                score = metric_func(scaled_data, cluster_labels)\n",
        "                results.append({'Metric': metric_name,\n",
        "                                'Dataset': dataset_label,\n",
        "                                'Method': algorithm_name,\n",
        "                                'Score': score})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "grouped = results_df.groupby(['Metric', 'Dataset', 'Method']).Score.mean().unstack()\n",
        "\n",
        "\n",
        "if os.path.exists(\"Mresults.xlsx\"):\n",
        "    os.remove(\"Mresults.xlsx\")\n",
        "\n",
        "grouped.to_excel(\"Mresults.xlsx\")\n",
        "print(grouped.to_latex())"
      ],
      "metadata": {
        "id": "7_DLG_Cs59q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating M-results (2) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"Mresults.xlsx\", engine='openpyxl')\n",
        "\n",
        "\n",
        "metric_columns = df.columns.difference(['Metric', 'Dataset'])\n",
        "\n",
        "\n",
        "updated_rows = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "\n",
        "    updated_rows = updated_rows.append(row)\n",
        "\n",
        "\n",
        "    if row['Dataset'] == 'D4':\n",
        "\n",
        "        last_4_rows = df.iloc[idx-3:idx+1][metric_columns]\n",
        "        avg_values = last_4_rows.mean()\n",
        "\n",
        "\n",
        "        avg_row_data = row.to_dict()\n",
        "        avg_row_data.update(avg_values)\n",
        "        avg_row_data['Dataset'] = 'Avg'\n",
        "\n",
        "\n",
        "        updated_rows = updated_rows.append(avg_row_data, ignore_index=True)\n",
        "\n",
        "\n",
        "df_updated = updated_rows.reset_index(drop=True)\n",
        "df_updated.to_excel(\"Mresults2.xlsx\", index=False)\n",
        "\n",
        "\n",
        "print(df_updated)"
      ],
      "metadata": {
        "id": "ny0Rc0by6XQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating M-results (3) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"Mresults2.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "\n",
        "averages = df.groupby(level=0).mean()\n",
        "\n",
        "\n",
        "for metric in averages.index:\n",
        "    df.loc[(metric, 'Avg'), :] = averages.loc[metric]\n",
        "\n",
        "# Determine rankings for the averages. Higher is better for Silhouette and Calinski-Harabasz,\n",
        "# while lower is better for Davies-Bouldin.\n",
        "metrics = df.index.get_level_values(0).unique()\n",
        "for metric in metrics:\n",
        "    if metric in [\"Silhouette\", \"Calinski-Harabasz\"]:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=False).astype(int)\n",
        "    else:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=True).astype(int)\n",
        "    df.loc[(metric, 'Rank'), :] = rank\n",
        "\n",
        "\n",
        "order = ['D1', 'D2', 'D3', 'D4', 'Avg', 'Rank']\n",
        "sorted_tuples = sorted(df.index, key=lambda x: (metrics.tolist().index(x[0]), order.index(x[1])))\n",
        "df = df.reindex(sorted_tuples)\n",
        "\n",
        "\n",
        "df.to_excel(\"Mresults.xlsx\")\n",
        "print(df.to_latex())\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Ga4uoR896wLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ** generating M-results (4) **\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample Data loading\n",
        "df = pd.read_excel(\"Mresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings = df.xs('Rank', level=1)\n",
        "\n",
        "colors = {\n",
        "    'Silhouette': 'green',\n",
        "    'Davies-Bouldin': 'blue',\n",
        "    'Calinski-Harabasz': 'orange'\n",
        "}\n",
        "\n",
        "algorithms_order = ['Agglomerative', 'BIRCH', 'DBSCAN', 'FCM', 'GMM', 'KMeans', 'KMedoids', 'OPTICS']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(rankings), figsize=(15, 5))\n",
        "\n",
        "\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "for ax, (metric, data) in zip(axes, rankings.iterrows()):\n",
        "    # Re-order the rankings such that 8 is at the bottom and 1 at the top\n",
        "    visual_rank = 9 - data.reindex(algorithms_order)  # 9 - rank to invert the bars\n",
        "    bars = ax.bar(visual_rank.index, visual_rank, color=colors[metric], width=0.5)\n",
        "\n",
        "    highest_rank_bar = data.idxmin()\n",
        "\n",
        "    # Highlight the highest rank bar with a red rectangle\n",
        "    for bar in bars:\n",
        "        if 9 - bar.get_height() == 1:  # find the rank 1 bar\n",
        "            rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "            ax.add_patch(rect)\n",
        "            break\n",
        "\n",
        "    ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim(0, 9)  # Set the y-axis limits\n",
        "    ax.set_yticks(range(1, 9))\n",
        "    ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])  # Explicitly setting the y-tick labels\n",
        "    ax.set_title(metric, fontweight='bold')\n",
        "    ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "plt.savefig('Mresults__evauation3.png', dpi=300)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "owxz5w4X7KTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kenemy Method for Aggregation"
      ],
      "metadata": {
        "id": "JCwqsQ4-5Eam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# --- Knemy - Young Method -----\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def kendall_tau_distance(rank_A, rank_B):\n",
        "    \"\"\"Calculate the number of pairwise disagreements between two rankings.\"\"\"\n",
        "    n = len(rank_A)\n",
        "    concordant = 0\n",
        "    discordant = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            a = rank_A[i] - rank_A[j]\n",
        "            b = rank_B[i] - rank_B[j]\n",
        "\n",
        "            if a * b > 0:\n",
        "                concordant += 1\n",
        "            elif a * b < 0:\n",
        "                discordant += 1\n",
        "\n",
        "    return discordant - concordant\n",
        "\n",
        "def kemeny_young_rankings(rankings):\n",
        "    \"\"\"Compute the Kemeny-Young method rankings.\"\"\"\n",
        "    aggregate_rankings = []\n",
        "    all_possible_rankings = list(itertools.permutations(rankings[0]))\n",
        "    for ranking in all_possible_rankings:\n",
        "        distance = sum(kendall_tau_distance(list(ranking), list(rank)) for rank in rankings)\n",
        "        aggregate_rankings.append((ranking, distance))\n",
        "    # The optimal ranking is the one with the smallest total distance\n",
        "    ky_ranking = min(aggregate_rankings, key=lambda x: x[1])[0]\n",
        "    return ky_ranking\n",
        "\n",
        "xls = pd.ExcelFile('aggregate-ranking.xlsx')\n",
        "\n",
        "# Titles mapping\n",
        "title_map = {\n",
        "    \"HM-ranking\": \"(c)Hardness+Modulus\",\n",
        "    \"H-ranking\": \"(a)Hardness\",\n",
        "    \"M-ranking\": \"(b)Modulus\"\n",
        "}\n",
        "\n",
        "# Desired order for plotting\n",
        "sheet_order = [\"H-ranking\", \"M-ranking\", \"HM-ranking\"]\n",
        "\n",
        "# Only consider the sheets in the desired order\n",
        "sheet_names = [sheet for sheet in sheet_order if sheet in xls.sheet_names]\n",
        "\n",
        "# Ordering of the algorithms\n",
        "algorithm_order = [\"Agglomerative\", \"BIRCH\", \"DBSCAN\", \"FCM\", \"GMM\", \"KMeans\", \"KMedoids\", \"OPTICS\"]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(sheet_names), figsize=(15, 5))\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "\n",
        "with pd.ExcelWriter('aggregate-ranking.xlsx', engine='openpyxl', mode='a') as writer:\n",
        "    for ax, sheet_name in zip(axes, sheet_names):\n",
        "        sheet_data = xls.parse(sheet_name)\n",
        "        rankings = sheet_data.iloc[1:].values.tolist()\n",
        "        aggregate_ranking = kemeny_young_rankings(rankings)\n",
        "        print(f'For {sheet_name}, the aggregated rank is {aggregate_ranking}')\n",
        "\n",
        "\n",
        "        aggregate_df = pd.DataFrame({\"Algorithm\": algorithm_order, \"Aggregate Rank\": aggregate_ranking})\n",
        "        aggregate_df.to_excel(writer, sheet_name=f\"{sheet_name}_AggregateRank\", index=False)\n",
        "\n",
        "\n",
        "        visual_data = pd.Series(aggregate_ranking, index=algorithm_order)\n",
        "        visual_rank = 9 - visual_data.reindex(algorithm_order)\n",
        "\n",
        "        bars = ax.bar(visual_rank.index, visual_rank, color='blue', width=0.5)\n",
        "        for bar in bars:\n",
        "            if 9 - bar.get_height() == 1:\n",
        "                rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "                ax.add_patch(rect)\n",
        "                break\n",
        "\n",
        "        ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylim(0, 9)\n",
        "        ax.set_yticks(range(1, 9))\n",
        "        ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])\n",
        "\n",
        "        # Set the title using the title_map dictionary\n",
        "        ax.set_title(title_map.get(sheet_name, sheet_name), fontweight='bold')\n",
        "\n",
        "        ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('aggregate_ranking_evaluation.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "xls = pd.ExcelFile('aggregate-ranking.xlsx')\n",
        "\n",
        "\n",
        "aggregate_sheets = ['H-ranking_AggregateRank', 'M-ranking_AggregateRank', 'HM-ranking_AggregateRank']\n",
        "aggregate_rankings = []\n",
        "\n",
        "for sheet in aggregate_sheets:\n",
        "    data = xls.parse(sheet)\n",
        "    ranking = data['Aggregate Rank'].tolist()\n",
        "    aggregate_rankings.append(ranking)\n",
        "\n",
        "\n",
        "aggregate_sheets = ['H-ranking_AggregateRank', 'M-ranking_AggregateRank', 'HM-ranking_AggregateRank']\n",
        "aggregate_rankings = []\n",
        "\n",
        "for sheet in aggregate_sheets:\n",
        "    data = xls.parse(sheet)\n",
        "    ranking = data['Aggregate Rank'].tolist()\n",
        "    aggregate_rankings.append(ranking)\n",
        "\n",
        "# Find the final order using the Kemeny-Young method\n",
        "final_order = kemeny_young_rankings(aggregate_rankings)\n",
        "print(f'The final aggregated rank is {final_order}')\n",
        "\n",
        "# Visualize the final order\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "plt.rcParams.update({'font.size': 12, 'font.weight': 'bold'})\n",
        "\n",
        "visual_data = pd.Series(final_order, index=algorithm_order)\n",
        "visual_rank = 9 - visual_data.reindex(algorithm_order)\n",
        "\n",
        "bars = ax.bar(visual_rank.index, visual_rank, color='blue', width=0.5)\n",
        "for bar in bars:\n",
        "    if 9 - bar.get_height() == 1:\n",
        "        rect = plt.Rectangle((bar.get_x() - 0.1, 0), bar.get_width() + 0.2, bar.get_height(), fill=False, edgecolor='red', linewidth=1.5)\n",
        "        ax.add_patch(rect)\n",
        "        break\n",
        "\n",
        "ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(0, 9)\n",
        "ax.set_yticks(range(1, 9))\n",
        "ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])\n",
        "ax.set_title('Final Ranking', fontweight='bold')\n",
        "ax.set_xticklabels(visual_rank.index, rotation=45, ha='right', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('final_ranking_evaluation.png', dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_X4sDIHKeH9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def kendall_tau_distance(rank_A, rank_B):\n",
        "    n = len(rank_A)\n",
        "    pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n",
        "    disagreements = 0\n",
        "    for x, y in pairs:\n",
        "        a = rank_A[x] - rank_A[y]\n",
        "        b = rank_B[x] - rank_B[y]\n",
        "        if a * b < 0:\n",
        "            disagreements += 1\n",
        "    return disagreements\n",
        "\n",
        "def kemeny_young(rankings):\n",
        "    n = len(rankings[0])\n",
        "    min_distance = float('inf')\n",
        "    best_ranking = None\n",
        "    for candidate in permutations(range(n)):\n",
        "        total_distance = sum(kendall_tau_distance(candidate, rank) for rank in rankings)\n",
        "        if total_distance < min_distance:\n",
        "            min_distance = total_distance\n",
        "            best_ranking = candidate\n",
        "    return best_ranking\n",
        "\n",
        "df = pd.read_excel(\"Hresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings_df = df.xs('Rank', level=1)\n",
        "methods_order = [\"Agglomerative\", \"BIRCH\", \"DBSCAN\",\"GMM\", \"FCM\", \"KMeans\", \"KMedoids\", \"OPTICS\" ]\n",
        "rankings = [list(row[method] for method in methods_order) for _, row in rankings_df.iterrows()]\n",
        "aggregate_ranking = [methods_order[i] for i in kemeny_young(rankings)]\n",
        "\n",
        "\n",
        "height_dict = {method: 9 - idx for idx, method in enumerate(aggregate_ranking)}\n",
        "\n",
        "\n",
        "bar_heights = [height_dict[method] for method in methods_order]\n",
        "\n",
        "hardness_results_df = pd.DataFrame({\n",
        "    'Clustering Methods': methods_order,\n",
        "    'Rank': [9 - height_dict[method] for method in methods_order]\n",
        "})\n",
        "\n",
        "\n",
        "hardness_results_df = hardness_results_df.sort_values(by='Rank')\n",
        "\n",
        "\n",
        "hardness_results_df.to_excel(\"hardness_ranking_result.xlsx\", index=False)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(range(len(methods_order)), bar_heights, color='blue')\n",
        "ax.set_xticks(range(len(methods_order)))\n",
        "ax.set_xticklabels(methods_order, rotation=90)\n",
        "\n",
        "max_height_idx = bar_heights.index(max(bar_heights))\n",
        "rect_width = 0.95\n",
        "rect_x_start = max_height_idx - (rect_width / 2)\n",
        "rect = Rectangle((rect_x_start, 0), rect_width, max(bar_heights), linewidth=1.5, edgecolor='r', facecolor='none', linestyle='dotted')\n",
        "ax.add_patch(rect)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Clustering Methods')\n",
        "ax.set_ylabel('Rank')\n",
        "ax.set_title('(a) Hardness')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDfAwBWsWuFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the ranking results for hardness, modulus, and hm\n",
        "hardness_df = pd.read_excel('hardness_rank_results.xlsx')\n",
        "modulus_df = pd.read_excel('modulus_rank_results.xlsx')\n",
        "hm_df = pd.read_excel('hm_rank_results.xlsx')\n",
        "\n",
        "# Transpose each dataframe so that the clustering methods become columns\n",
        "hardness_df = hardness_df.T\n",
        "modulus_df = modulus_df.T\n",
        "hm_df = hm_df.T\n",
        "\n",
        "# Extract the ranks for each attribute into a new dataframe\n",
        "data = {\n",
        "    hardness_df.columns[0]: hardness_df.iloc[0].values,\n",
        "    modulus_df.columns[0]: modulus_df.iloc[0].values,\n",
        "    hm_df.columns[0]: hm_df.iloc[0].values\n",
        "}\n",
        "\n",
        "combined_df = pd.DataFrame(data)\n",
        "\n",
        "# Set the index to the names of the clustering methods\n",
        "combined_df.index = hardness_df.index\n",
        "\n",
        "# Transpose the final dataframe so that clustering methods are column headers\n",
        "combined_df = combined_df.transpose()\n",
        "\n",
        "combined_df.to_excel('ready_for_ranking.xlsx')\n",
        "\n"
      ],
      "metadata": {
        "id": "E9rxMGWz0w-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_values_for_metric(filename, metric_row):\n",
        "    \"\"\"Read the file, and return values for the given metric from the specified row.\"\"\"\n",
        "    data = pd.read_excel(filename)\n",
        "\n",
        "\n",
        "    values = data.iloc[metric_row, 2:10].values\n",
        "    return values\n",
        "\n",
        "files = ['Hresults.xlsx', 'HMresults.xlsx', 'Mresults.xlsx']\n",
        "colors = ['blue', 'red', 'black']\n",
        "label_mapper = {\n",
        "    'Hresults': 'Hardness',\n",
        "    'HMresults': 'Hardness + Modulus',\n",
        "    'Mresults': 'Modulus'\n",
        "}\n",
        "file_labels = [label_mapper[filename.split('.')[0]] for filename in files]\n",
        "\n",
        "\n",
        "data_sample = pd.read_excel(files[0])\n",
        "cluster_names = data_sample.columns[2:10]\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': 16,\n",
        "    'Calinski-Harabasz': 4,\n",
        "    'Davies-Bouldin': 10\n",
        "}\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 6), constrained_layout=True)\n",
        "\n",
        "for ax, (metric_name, metric_row) in zip(axs, metrics.items()):\n",
        "\n",
        "    for filename, color, label in zip(files, colors, file_labels):\n",
        "        values = get_values_for_metric(filename, metric_row)\n",
        "        ax.plot(cluster_names, values, marker='o', color=color, linewidth=2, label=label)\n",
        "\n",
        "    ax.set_title(metric_name)\n",
        "    ax.set_xticklabels(cluster_names, rotation=45, fontsize=10, fontname='sans')\n",
        "    ax.grid(True)\n",
        "\n",
        "\n",
        "handles, labels = axs[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TmxJWFgIw2Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------\n",
        "#----------------------------------\n",
        "#----------------------------------\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import permutations\n",
        "\n",
        "def kendall_tau_distance(rank_a, rank_b):\n",
        "    \"\"\"Calculate the Kendall-Tau distance between two rankings.\"\"\"\n",
        "    pairs = [(a, b) for idx, a in enumerate(rank_a) for b in rank_a[idx + 1:]]\n",
        "    distance = 0\n",
        "    for x, y in pairs:\n",
        "        a = rank_a.index(x) - rank_a.index(y)\n",
        "        b = rank_b.index(x) - rank_b.index(y)\n",
        "        if a * b < 0:\n",
        "            distance += 1\n",
        "    return distance\n",
        "\n",
        "def kemeny_young(rankings):\n",
        "    \"\"\"Compute the Kemeny-Young optimal aggregation ranking.\"\"\"\n",
        "    min_distance = float('inf')\n",
        "    best_ranking = None\n",
        "\n",
        "    for candidate in permutations(rankings[0]):\n",
        "        distance = sum(kendall_tau_distance(candidate, rank) for rank in rankings)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            best_ranking = candidate\n",
        "\n",
        "    return best_ranking\n",
        "\n",
        "# Evaluation ranking for HARDNESS / Calinki - Davies - Silhouette\n",
        "rankings_Hardness = [\n",
        "    ['KMeans', 'FCM', 'KMedoids' , 'Agglomerative', 'GMM', 'BIRCH' , 'DBSCAN' , 'OPTICS'],\n",
        "    ['BIRCH', 'KMeans', 'Agglomerative', 'FCM' , 'KMedoids' , 'GMM', 'DBSCAN' , 'OPTICS'],\n",
        "    ['DBSCAN', 'BIRCH' , 'KMeans' , 'FCM' , 'GMM' , 'Agglomerative' , 'KMedoids' , 'OPTICS']\n",
        "]\n",
        "\n",
        "print(\" final ranking for HARDNESS is : \")\n",
        "print(kemeny_young(rankings_Hardness))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation ranking for MODULUS / Calinki - Davies - Silhouette\n",
        "rankings_Modulus = [\n",
        "    ['KMeans', 'FCM', 'KMedoids' , 'Agglomerative', 'GMM', 'BIRCH' , 'DBSCAN' , 'OPTICS'],\n",
        "    ['BIRCH', 'KMeans', 'Agglomerative', 'FCM' , 'KMedoids' , 'GMM', 'DBSCAN' , 'OPTICS'],\n",
        "    ['DBSCAN', 'KMeans' , 'FCM' , 'BIRCH' ,  'GMM' , 'Agglomerative' , 'KMedoids' , 'OPTICS']\n",
        "]\n",
        "\n",
        "print(\" finalranking for MODULUS is : \")\n",
        "print(kemeny_young(rankings_Modulus))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation ranking for MODULUS / Calinki - Davies - Silhouette\n",
        "rankings_HM = [\n",
        "    ['KMeans', 'FCM', 'KMedoids' , 'Agglomerative', 'GMM', 'BIRCH' , 'DBSCAN' , 'OPTICS'],\n",
        "    ['BIRCH', 'Agglomerative', 'KMeans','FCM' , 'KMedoids' , 'GMM', 'DBSCAN' , 'OPTICS'],\n",
        "    ['GMM', 'KMeans' , 'Agglomerative' , 'BIRCH' ,  'FCM' , 'KMedoids' , 'DBSCAN', 'OPTICS']\n",
        "]\n",
        "\n",
        "print(\" finalranking for HM is : \")\n",
        "print(kemeny_young(rankings_HM))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ranking(rankings, titles):\n",
        "    # Define the fixed order\n",
        "    order = ['Agglomerative', 'BIRCH', 'DBSCAN', 'FCM', 'GMM', 'KMeans', 'KMedoids', 'OPTICS']\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(6, 18))\n",
        "\n",
        "    for idx, (ranking, metric) in enumerate(zip(rankings, titles)):\n",
        "        ax = axes[idx]\n",
        "        # Compute the ranking indices based on the provided order\n",
        "        indices = [8 - ranking.index(method) for method in order]  # adjust the order to match the y-axis labels\n",
        "\n",
        "        ax.bar(order, indices, color='blue')\n",
        "       # ax.set_xlabel('Clustering Methods', fontweight='bold')\n",
        "        ax.set_title(metric, fontweight='bold')\n",
        "        ax.set_xticks(order)  # Set x-ticks based on clustering methods\n",
        "        ax.set_xticklabels(order, rotation=45, ha='right', fontweight='bold')\n",
        "        ax.set_ylabel('Rank', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylim(0, 9)  # Set the y-axis limits\n",
        "        ax.set_yticks(range(1, 9))\n",
        "        ax.set_yticklabels(['8', '7', '6', '5', '4', '3', '2', '1'])  # Explicitly setting the y-tick labels\n",
        "        ax.grid(axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('aggregate_results.png', dpi=500)\n",
        "    plt.show()\n",
        "\n",
        "# List of rankings\n",
        "all_rankings = [\n",
        "    kemeny_young(rankings_HM),\n",
        "    kemeny_young(rankings_Hardness),\n",
        "    kemeny_young(rankings_Modulus)\n",
        "\n",
        "]\n",
        "\n",
        "# Titles for each ranking plot\n",
        "all_titles = [\"(a)Ranking for Hardness & Modulus\", \"(b)Ranking for HARDNESS\", \"(c)Ranking for MODULUS\", ]\n",
        "\n",
        "# Plot the rankings\n",
        "plot_ranking(all_rankings, all_titles)\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Apply the Kemeny-Young method to the aggregated list to get the final consensus ranking\n",
        "final_ranking = kemeny_young(all_rankings)\n",
        "\n",
        "print(\"Aggregated Kemeny-Young Ranking:\")\n",
        "print(final_ranking)\n",
        "plot_single_ranking(final_ranking, \"Aggregated Kemeny-Young Ranking\")"
      ],
      "metadata": {
        "id": "f8DjY0hNY2jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-HvOZ0ZPKLPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "name": "evaluation_modulus4.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}