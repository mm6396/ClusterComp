{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mm6396/ClusterComp/blob/main/evaluation_modulus4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHucI84mXoGi"
      },
      "source": [
        "##**Imports Section**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj4a7fr6XoGi"
      },
      "outputs": [],
      "source": [
        "!pip install fuzzy-c-means\n",
        "!pip install scikit-learn-extra\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy import interpolate\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "from fcmeans import FCM\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.cluster import kmeans_plusplus\n",
        "from patsy import dmatrices\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import seaborn as sn\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "file_path = \"test2.xlsx\"\n",
        "sheet_name = \"Test 1\"\n",
        "use_cols = \"A:C\"\n",
        "hard_col_name = \"HARDNESS\"\n",
        "x_col_name = \"X Position\"\n",
        "y_col_name = \"Y Position\"\n",
        "modulus_col_name = \"MODULUS\"\n",
        "NUM_HIST_BINS = 200\n",
        "nulls = False\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeiJcGQeXoGj"
      },
      "source": [
        "##**Variables and Constants**\n",
        "This is the section Setting up any variables or constants used throughout the program. This is where you'll want to set things like the filename and any constants you use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiHibg-UXoGk"
      },
      "outputs": [],
      "source": [
        "xls1 = pd.read_excel('test1.xlsx')\n",
        "xls2 = pd.read_excel('test2.xlsx')\n",
        "\n",
        "xls3 = pd.read_excel('test3.xlsx')\n",
        "xls4 = pd.read_excel('test4.xlsx')\n",
        "\n",
        "#if nulls:\n",
        "  #  dxl = pd.read_excel(xls, sheet_name, usecols=use_cols).iloc[:-1]\n",
        "#else:\n",
        "  #  dxl = pd.read_excel(xls, sheet_name, usecols=use_cols).dropna()\n",
        "#print(xls)\n",
        "corr_matrix1 = xls1.corr()\n",
        "corr_matrix2 = xls2.corr()\n",
        "corr_matrix3 = xls3.corr()\n",
        "corr_matrix4 = xls4.corr()\n",
        "\n",
        "sn.heatmap(corr_matrix1, annot=True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "sn.heatmap(corr_matrix2, annot=True)\n",
        "plt.show()\n",
        "\n",
        "sn.heatmap(corr_matrix3, annot=True)\n",
        "plt.show()\n",
        "\n",
        "sn.heatmap(corr_matrix4, annot=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqPpRYDiXoGl"
      },
      "source": [
        "##**Organizing Data and Making it Numeric**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "implVt7sXoGl",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# If skip_first_row then read every row after the first, otherwise read them all\n",
        "# The hardness values\n",
        "hardness_column = xls4[hard_col_name]\n",
        "# The x values\n",
        "x_column = xls4[x_col_name]\n",
        "# The y values\n",
        "y_column = xls4[y_col_name]\n",
        "modulus_column=xls4[modulus_col_name]\n",
        "# The stiffness column\n",
        "# Asserts that the columns have values\n",
        "assert hardness_column is not None\n",
        "assert x_column is not None\n",
        "assert y_column is not None\n",
        "assert modulus_column is not None\n",
        "\n",
        "hard_df = pd.DataFrame(hardness_column)\n",
        "modulus_df=pd.DataFrame(modulus_column)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(modulus_df.describe())\n",
        "fig=px.box(modulus_df, y='MODULUS')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "fig=px.box(modulus_df, y='MODULUS')\n",
        "fig.show()\n",
        "\n",
        "print(\"hm_df is  :\" ,modulus_df)\n",
        "x_df = pd.DataFrame(x_column)\n",
        "y_df = pd.DataFrame(y_column)\n",
        "\n",
        "# Renames columns of all read data for consistency\n",
        "#hard_df = hard_df.rename(columns={hard_col_name: \"Data\"})\n",
        "\n",
        "#modulus_df=modulus_df.rename(columns={modulus_col_name:\"Data\"})\n",
        "\n",
        "#hm_df=modulus_df.rename(columns={modulus_col_name:\"Data\" ,hard_col_name: \"Data\" })\n",
        "#print(hm_df[\"Data\"])\n",
        "\n",
        "x_df = x_df.rename(columns={x_col_name: \"Data\"})\n",
        "y_df = y_df.rename(columns={y_col_name: \"Data\"})\n",
        "\n",
        "\n",
        "# Makes all read data numeric\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Befor removing outliers"
      ],
      "metadata": {
        "id": "tW7OnK8i38i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(modulus_df)"
      ],
      "metadata": {
        "id": "qXkJX4ej347E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWAPIFusXoGm"
      },
      "source": [
        "##**Data Clean - Setting Nulls** and interpolating\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Highest_allowed_Modulus=modulus_df['MODULUS'].mean() + 3*modulus_df['MODULUS'].std()\n",
        "Lowest_allowed_Modulus=modulus_df['MODULUS'].mean() - 3*modulus_df['MODULUS'].std()\n",
        "\n",
        "\n",
        "print(\"Highest allowed Modulus\",Highest_allowed_Modulus)\n",
        "print(\"Lowest allowed Modulus\",Lowest_allowed_Modulus)\n",
        "\n",
        "\n",
        "#Step-5: Finding the Outliers\n",
        "#new_hm_df11=hm_df[(hm_df['MODULUS'] < Highest_allowed_Modulus) | (hm_df['MODULUS'] > Lowest_allowed_Modulus)].append(hm_df[(hm_df['HARDNESS'] < Highest_allowed_hardness) | (hm_df['HARDNESS'] > Lowest_allowed_hardness)])\n",
        "#print(new_hm_df11)\n",
        "#plot outliers\n",
        "\n",
        "#plt.scatter(new_hm_df11['HARDNESS'],new_hm_df11['MODULUS'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "new_modulus_df = np.where(\n",
        "    modulus_df ['MODULUS']> Highest_allowed_Modulus,\n",
        "    modulus_df['MODULUS'].mean() , modulus_df['MODULUS'])\n",
        "new_modulus_df=np.where(\n",
        "        modulus_df['MODULUS']< Lowest_allowed_Modulus,\n",
        "       modulus_df['MODULUS'].mean(),\n",
        "        modulus_df['MODULUS']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(new_modulus_df)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-QNcO_aVut94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyKr5rgrXoGp"
      },
      "source": [
        "#**Clustering the Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN clustering"
      ],
      "metadata": {
        "id": "75h1OA4Wd76r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "import time\n",
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number !\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "\n",
        "print('\\n <><><><><><><><><><> \"  After cleaning  \"<><><><><><><><><><><>\\n\\n')\n",
        "print(new_modulus_df)\n",
        "\n",
        "print(np.shape(new_modulus_df))\n",
        "db = DBSCAN(eps=0.69, min_samples=100).fit(new_modulus_df['MODULUS'].values.reshape(-1,1))\n",
        "labels = db.labels_\n",
        "\n",
        "print(\"These are DBSCAN_labels: \\n\" , labels, len(labels)  , new_modulus_df)\n",
        "\n",
        "\n",
        "print('Sihhuette score ', num_clusters ,' is' , silhouette_score(new_modulus_df,labels))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, labels))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, labels))\n",
        "\n",
        "print('\\n <><><><><><><><><><> \"  Before \"<><><><><><><><><><><>\\n')\n",
        "modulus_df = modulus_df.dropna()\n",
        "print(modulus_df)\n",
        "db = DBSCAN(eps=0.69, min_samples=100).fit(modulus_df[:])\n",
        "labels = db.labels_\n",
        "from collections import Counter\n",
        "Counter(labels)\n",
        "\n",
        "print(\"These are DBSCAN_labels: \\n\" , labels, len(labels)  , modulus_df)\n",
        "\n",
        "print('Sihhuette score ', num_clusters ,' is' , silhouette_score(modulus_df,labels))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, labels))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, labels))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ugcf8qfmhMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahokFC07zYxv"
      },
      "source": [
        "# **This is Fuzzy c-means**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IMOC33szqr7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from fcmeans import FCM\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number !\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "new_hm_df= new_modulus_df.dropna()\n",
        "fcm = FCM(n_clusters= 3, Fuzzifier= 2)\n",
        "print(new_hm_df)\n",
        "fcm.fit(new_modulus_df.to_numpy())\n",
        "\n",
        "#fcm_centers_cleaned = fcm.centers\n",
        "\n",
        "fcm_labels_cleaned = fcm.predict(new_modulus_df.to_numpy())\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(new_modulus_df,fcm_labels_cleaned))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, fcm_labels_cleaned))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, fcm_labels_cleaned))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n ')\n",
        "modulus_df= modulus_df.dropna()\n",
        "fcm = FCM(n_clusters=num_clusters , Fuzzifier= 2)\n",
        "fcm.fit(new_modulus_df.to_numpy())\n",
        "\n",
        "#fcm_centers_cleaned = fcm.centers\n",
        "print(\"fcm centers are :\",fcm_centers_cleaned)\n",
        "fcm_labels_cleaned = fcm.predict(modulus_df.to_numpy())\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(modulus_df,fcm_labels_cleaned))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, fcm_labels_cleaned))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, fcm_labels_cleaned))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeM71IwHrGA"
      },
      "source": [
        "#**This is KMeans clustering**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number!\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "k_means = KMeans(n_clusters=num_clusters, random_state=0, tol=0.0001, verbose=0.5, algorithm='auto').fit(new_modulus_df)\n",
        "k_means_labels_cleaned = k_means.labels_\n",
        "\n",
        "\n",
        "\n",
        "print('Sihhuette score for number of couters ' , num_clusters ,' is' , silhouette_score(new_modulus_df,k_means.labels_))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(new_modulus_df, k_means_labels_cleaned))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, k_means_labels_cleaned))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "k_means = KMeans(n_clusters=num_clusters, random_state=0 , tol=0.0001, verbose=0.5, algorithm='auto').fit(hm_df)\n",
        "k_means_labels_cleaned = k_means.labels_\n",
        "\n",
        "\n",
        "\n",
        "print('Sihhuette score for number of clusters ' , num_clusters ,' is' , silhouette_score(modulus_df,k_means.labels_))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(modulus_df, k_means_labels_cleaned))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, k_means_labels_cleaned))\n"
      ],
      "metadata": {
        "id": "FGiNEDdss7S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG0rbXpKcfXG"
      },
      "source": [
        "#**This is GuassianMixture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLUqp-bnVchJ"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"The number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number at all! This is a string. go and try again. Better luck next time!\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "gmm= GaussianMixture(n_components= num_clusters)\n",
        "gmm.fit(new_modulus_df)\n",
        "\n",
        "gmm_results = gmm.predict(new_modulus_df)\n",
        "\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(new_modulus_df,gmm_results))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, gmm_results))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, gmm_results))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "gmm= GaussianMixture(n_components= num_clusters)\n",
        "gmm.fit(modulus_df)\n",
        "\n",
        "gmm_results = gmm.predict(modulus_df)\n",
        "\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(modulus_df,gmm_results))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, gmm_results))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, gmm_results))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**This is Agglomorative Clustering**"
      ],
      "metadata": {
        "id": "rC53WjZ13Wgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0H371AI6Ua7"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering # this line of code imports AgglomerativeClustering model from sk-learn\n",
        "'''\n",
        "we need to create an AgglomerativeClustering object, and in it, we pass the following parameters:\n",
        "n_cluster= 5, the number of clusters our model should return\n",
        "affinity=euclidean, specify metric to be used to calculate distances\n",
        "linkage= ward to regulate how distance calculation will be carried out between different clusters.\n",
        "'''\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "num_clusters = 3\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "Agg_hc = AgglomerativeClustering(n_clusters = num_clusters,  linkage = 'complete')\n",
        "y_hc = Agg_hc.fit_predict(new_modulus_df)# model fitting on the dataset\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(new_modulus_df,y_hc))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(new_modulus_df, y_hc))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, y_hc))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "Agg_hc = AgglomerativeClustering(n_clusters = num_clusters , linkage = 'complete')\n",
        "y_hc = Agg_hc.fit_predict(modulus_df)# model fitting on the dataset\n",
        "\n",
        "\n",
        "\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(modulus_df,y_hc))\n",
        "\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, y_hc))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, y_hc))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectral clustering"
      ],
      "metadata": {
        "id": "BK0-ApcsJS_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**This is Birch Clustering**"
      ],
      "metadata": {
        "id": "bLAz0VF43fnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "birch_clust=Birch(branching_factor=100, threshold =0.25).fit(new_modulus_df)\n",
        "birch_results = birch_clust.predict(new_modulus_df)\n",
        "\n",
        "clustered_df = pd.DataFrame(birch_results)\n",
        "#clustered_df.columns = [\"Data\"]\n",
        "num_clusters = 3\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(new_modulus_df,birch_results))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, birch_results))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, birch_results))\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "birch_clust=Birch(branching_factor=100, threshold =0.25).fit(modulus_df)\n",
        "birch_results = birch_clust.predict(modulus_df)\n",
        "\n",
        "clustered_df = pd.DataFrame(birch_results)\n",
        "#clustered_df.columns = [\"Data\"]\n",
        "num_clusters = 3\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(modulus_df,birch_results))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, birch_results))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, birch_results))\n",
        "\n"
      ],
      "metadata": {
        "id": "rWoslMsT3gd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**This is Optics Clustering**"
      ],
      "metadata": {
        "id": "bvvxkfVK-Jue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "import time\n",
        "num_clusters =3\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "optics_clustering = OPTICS(min_samples=2 , max_eps = 0.9).fit(new_modulus_df)\n",
        "\n",
        "optics_labels=optics_clustering.labels_\n",
        "optics_results = optics_clustering.fit_predict(new_modulus_df)\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(new_modulus_df,optics_labels))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(new_modulus_df, optics_labels))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, optics_labels))\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "optics_clustering = OPTICS(min_samples=2 , max_eps = 0.9).fit(modulus_df)\n",
        "optics_clustering_labels=optics_clustering.labels_\n",
        "optics_results = optics_clustering.fit_predict(modulus_df)\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(modulus_df,optics_results))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(modulus_df, optics_results))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, optics_results))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egJ_z79d-KAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implementing kmedoid clustering ***"
      ],
      "metadata": {
        "id": "JS4UZnI2zIuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number !\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "k_medoids= KMedoids(n_clusters=num_clusters).fit(new_modulus_df)\n",
        "k_medoids_labels_cleaned = k_medoids.labels_\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(new_modulus_df,k_medoids_labels_cleaned))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, k_medoids_labels_cleaned))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, k_medoids_labels_cleaned))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "k_medoids= KMedoids(n_clusters=num_clusters).fit(modulus_df)\n",
        "k_medoids_labels_cleaned = k_medoids.labels_\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(modulus_df,k_medoids_labels_cleaned))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulu_df, k_medoids_labels_cleaned))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, k_medoids_labels_cleaned))\n",
        "\n"
      ],
      "metadata": {
        "id": "5l6_0qnyzH_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***New Expremiments; ***"
      ],
      "metadata": {
        "id": "0UPGXTKg1FnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing datasets :\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def processing_procedure(files):\n",
        "    processed_files = []\n",
        "    for filename in files:\n",
        "\n",
        "        df = pd.read_excel(filename)\n",
        "\n",
        "\n",
        "        df.interpolate(method='nearest', inplace=True, limit_direction='both')\n",
        "\n",
        "\n",
        "        processed_filename = filename.split('.')[0] + '_processed.' + filename.split('.')[1]\n",
        "\n",
        "\n",
        "        if os.path.exists(processed_filename):\n",
        "            print(f\"{processed_filename} already exists. Overwriting...\")\n",
        "\n",
        "        # Save (or overwrite) the processed data into the new spreadsheet.\n",
        "        df.to_excel(processed_filename, index=False)\n",
        "\n",
        "        processed_files.append(processed_filename)\n",
        "\n",
        "    return processed_files\n"
      ],
      "metadata": {
        "id": "U_J6KpxiyLut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['HARDNESS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Hardness')\n",
        "\n",
        "plt.suptitle('Hardness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p5RBL3wOZ5EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardness in #D3 dimension\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming the processing_procedure is already defined\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "for idx, (df, file) in enumerate(zip(datasets, files_p)):\n",
        "    ax = fig.add_subplot(1, len(datasets), idx+1, projection='3d')\n",
        "    sc = ax.scatter3D(df['X Position'], df['Y Position'], df['Z Position'], c=df['HARDNESS'], cmap='viridis', s=50)\n",
        "\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    ax.set_ylabel('Y Position')\n",
        "    ax.set_zlabel('Z Position')\n",
        "\n",
        "fig.colorbar(sc, ax=axs, fraction=0.01, pad=0.1, label='Hardness')\n",
        "plt.suptitle('Hardness based on X, Y, and Z Position')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HXcJxOpW-32H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['MODULUS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Modulus')\n",
        "\n",
        "plt.suptitle('Modulus based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6H0QYln6a75U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Stiffness'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Stiffness')\n",
        "plt.suptitle('Stiffness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jA0by-6JbK3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Load'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Load')\n",
        "\n",
        "plt.suptitle('Load based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tJll6_92be9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulizing Data\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop('Depth', axis=1) for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering methods\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "# Visualization\n",
        "n_rows = len(scaled_datasets)\n",
        "n_cols = len(clustering_algorithms)\n",
        "\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 15))\n",
        "\n",
        "for row, scaled_data in enumerate(scaled_datasets):\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for col, (algorithm_name, algorithm) in enumerate(clustering_algorithms.items()):\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        axs[row, col].scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        axs[row, col].set_title(f'Dataset {row + 1} using {algorithm_name}')\n",
        "\n",
        "\n",
        "        if row == 0:\n",
        "            axs[row, col].set_xlabel(algorithm_name)\n",
        "        if col == 0:\n",
        "            axs[row, col].set_ylabel(f'Dataset {row + 1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7BGrLWvwdHkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means"
      ],
      "metadata": {
        "id": "yRxJH-qcoQTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness', 'Modulus'] # please exclude all features that you dont need it\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': silhouette_score,\n",
        "    'Calinski-Harabasz': calinski_harabasz_score,\n",
        "    'Davies-Bouldin': davies_bouldin_score\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                score = metric_func(scaled_data, cluster_labels)\n",
        "                results.append({'Metric': metric_name,\n",
        "                                'Dataset': dataset_label,\n",
        "                                'Method': algorithm_name,\n",
        "                                'Score': score})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "grouped = results_df.groupby(['Metric', 'Dataset', 'Method']).Score.mean().unstack()\n",
        "\n",
        "\n",
        "if os.path.exists(\"results.xlsx\"):\n",
        "    os.remove(\"results.xlsx\")\n",
        "\n",
        "grouped.to_excel(\"results.xlsx\")\n",
        "print(grouped.to_latex())\n",
        "\n",
        "\n",
        "\n",
        "print(grouped.to_latex())\n",
        "\n"
      ],
      "metadata": {
        "id": "sF2jU4MxBCl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"results.xlsx\", engine='openpyxl')\n",
        "\n",
        "\n",
        "metric_columns = df.columns.difference(['Metric', 'Dataset'])\n",
        "\n",
        "\n",
        "updated_rows = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "\n",
        "    updated_rows = updated_rows.append(row)\n",
        "\n",
        "\n",
        "    if row['Dataset'] == 'D4':\n",
        "\n",
        "        last_4_rows = df.iloc[idx-3:idx+1][metric_columns]\n",
        "        avg_values = last_4_rows.mean()\n",
        "\n",
        "\n",
        "        avg_row_data = row.to_dict()\n",
        "        avg_row_data.update(avg_values)\n",
        "        avg_row_data['Dataset'] = 'Avg'\n",
        "\n",
        "\n",
        "        updated_rows = updated_rows.append(avg_row_data, ignore_index=True)\n",
        "\n",
        "\n",
        "df_updated = updated_rows.reset_index(drop=True)\n",
        "df_updated.to_excel(\"results2.xlsx\", index=False)\n",
        "\n",
        "\n",
        "print(df_updated)"
      ],
      "metadata": {
        "id": "hSPKo5kcGrfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"results2.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "\n",
        "averages = df.groupby(level=0).mean()\n",
        "\n",
        "# Insert the averages to the original dataframe\n",
        "for metric in averages.index:\n",
        "    df.loc[(metric, 'Avg'), :] = averages.loc[metric]\n",
        "\n",
        "# Determine rankings for the averages. Higher is better for Silhouette and Calinski-Harabasz,\n",
        "# while lower is better for Davies-Bouldin.\n",
        "metrics = df.index.get_level_values(0).unique()\n",
        "for metric in metrics:\n",
        "    if metric in [\"Silhouette\", \"Calinski-Harabasz\"]:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=False).astype(int)\n",
        "    else:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=True).astype(int)\n",
        "    df.loc[(metric, 'Rank'), :] = rank\n",
        "\n",
        "\n",
        "order = ['D1', 'D2', 'D3', 'D4', 'Avg', 'Rank']\n",
        "sorted_tuples = sorted(df.index, key=lambda x: (metrics.tolist().index(x[0]), order.index(x[1])))\n",
        "df = df.reindex(sorted_tuples)\n",
        "\n",
        "\n",
        "df.to_excel(\"results.xlsx\")\n",
        "print(df.to_latex())\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "vUvKPOuYKUO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the DataFrame\n",
        "df = pd.read_excel(\"results.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "# Filter rows where the second index is 'Rank'\n",
        "rankings = df.xs('Rank', level=1)\n",
        "\n",
        "# Define the colors and the mapping\n",
        "colors = {\n",
        "    'Silhouette': 'green',\n",
        "    'Davies-Bouldin': 'blue',\n",
        "    'Calinski-Harabasz': 'orange'\n",
        "}\n",
        "\n",
        "# Define the order for algorithms\n",
        "algorithms_order = ['KMeans', 'KMedoids', 'FCM', 'Agglomerative', 'BIRCH', 'DBSCAN', 'OPTICS']\n",
        "\n",
        "# Plotting the bar chart for each evaluation metric\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(rankings), figsize=(15, 5))\n",
        "\n",
        "for ax, (metric, data) in zip(axes, rankings.iterrows()):\n",
        "    adjusted_rank = 9 - data.reindex(algorithms_order)  # To make Rank 1 have the highest height\n",
        "    bars = adjusted_rank.plot(kind='bar', ax=ax, title=metric, color=colors[metric], width=0.5)\n",
        "\n",
        "    # Determine which bar has the highest rank based on the original 'Rank' data\n",
        "    highest_rank_bar = data.idxmin()\n",
        "\n",
        "    # Retrieve details about the bar to be highlighted\n",
        "    bar_width = bars.patches[0].get_width()\n",
        "    bar_x = bars.patches[adjusted_rank.index.get_loc(highest_rank_bar)].get_x()\n",
        "    bar_height = bars.patches[adjusted_rank.index.get_loc(highest_rank_bar)].get_height()\n",
        "\n",
        "    # Create the rectangle around the highest rank bar\n",
        "    rect = plt.Rectangle((bar_x - 0.1, 0), bar_width + 0.2, bar_height + 0.1, fill=False, edgecolor='red', linewidth=1.5)\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    ax.set_ylabel('Rank')\n",
        "    ax.set_ylim(0, 9)  # Extend ylim to account for the rectangle's height\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Di7GXnIJPJ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def kendall_tau_distance(rank_A, rank_B):\n",
        "    n = len(rank_A)\n",
        "    pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n",
        "    disagreements = 0\n",
        "    for x, y in pairs:\n",
        "        a = rank_A[x] - rank_A[y]\n",
        "        b = rank_B[x] - rank_B[y]\n",
        "        if a * b < 0:\n",
        "            disagreements += 1\n",
        "    return disagreements\n",
        "\n",
        "def kemeny_young(rankings):\n",
        "    n = len(rankings[0])\n",
        "    min_distance = float('inf')\n",
        "    best_ranking = None\n",
        "    for candidate in permutations(range(n)):\n",
        "        total_distance = sum(kendall_tau_distance(candidate, rank) for rank in rankings)\n",
        "        if total_distance < min_distance:\n",
        "            min_distance = total_distance\n",
        "            best_ranking = candidate\n",
        "    return best_ranking\n",
        "\n",
        "df = pd.read_excel(\"results.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings_df = df.xs('Rank', level=1)\n",
        "methods_order = [\"KMeans\", \"KMedoids\", \"FCM\", \"Agglomerative\", \"BIRCH\", \"DBSCAN\", \"OPTICS\"]\n",
        "rankings = [list(row[method] for method in methods_order) for _, row in rankings_df.iterrows()]\n",
        "aggregate_ranking = [methods_order[i] for i in kemeny_young(rankings)]\n",
        "\n",
        "# Create a dictionary for bar heights based on aggregate ranking\n",
        "height_dict = {method: 9 - idx for idx, method in enumerate(aggregate_ranking)}\n",
        "\n",
        "# Extract the heights in the desired order\n",
        "bar_heights = [height_dict[method] for method in methods_order]\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(range(len(methods_order)), bar_heights, color='blue')\n",
        "ax.set_xticks(range(len(methods_order)))\n",
        "ax.set_xticklabels(methods_order, rotation=90)\n",
        "\n",
        "max_height_idx = bar_heights.index(max(bar_heights))\n",
        "rect_width = 0.95\n",
        "rect_x_start = max_height_idx - (rect_width / 2)\n",
        "rect = Rectangle((rect_x_start, 0), rect_width, max(bar_heights), linewidth=1.5, edgecolor='r', facecolor='none', linestyle='dotted')\n",
        "ax.add_patch(rect)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Clustering Methods')\n",
        "ax.set_ylabel('Rank')\n",
        "ax.set_title('Final Aggregate Ranking')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDfAwBWsWuFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the datasets\n",
        "files = [\"test3.xlsx\"]\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "\n",
        "for index, scaled_data in enumerate(scaled_datasets, 1):\n",
        "    pca = PCA(n_components=3)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 6))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        scatter = ax.scatter(data_pca[:, 0], data_pca[:, 1], data_pca[:, 2], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        ax.set_xlabel('First Principal Component')\n",
        "        ax.set_ylabel('Second Principal Component')\n",
        "        ax.set_zlabel('Third Principal Component')\n",
        "        ax.set_title(f'PCA Cluster Visualization for Dataset {index} using {algorithm_name}')\n",
        "        fig.colorbar(scatter)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "wN7iSDkBmzrM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "name": "evaluation_modulus3.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}