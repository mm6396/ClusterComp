{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mm6396/ClusterComp/blob/main/evaluation_modulus4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHucI84mXoGi"
      },
      "source": [
        "##**Imports Section**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***New Expremiments; ***"
      ],
      "metadata": {
        "id": "0UPGXTKg1FnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing datasets :\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def processing_procedure(files):\n",
        "    processed_files = []\n",
        "    for filename in files:\n",
        "\n",
        "        df = pd.read_excel(filename)\n",
        "\n",
        "\n",
        "        df.interpolate(method='nearest', inplace=True, limit_direction='both')\n",
        "\n",
        "\n",
        "        processed_filename = filename.split('.')[0] + '_processed.' + filename.split('.')[1]\n",
        "\n",
        "\n",
        "        if os.path.exists(processed_filename):\n",
        "            print(f\"{processed_filename} already exists. Overwriting...\")\n",
        "\n",
        "        # Save (or overwrite) the processed data into the new spreadsheet.\n",
        "        df.to_excel(processed_filename, index=False)\n",
        "\n",
        "        processed_files.append(processed_filename)\n",
        "\n",
        "    return processed_files\n"
      ],
      "metadata": {
        "id": "U_J6KpxiyLut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['HARDNESS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Hardness')\n",
        "\n",
        "plt.suptitle('Hardness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p5RBL3wOZ5EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['MODULUS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Modulus')\n",
        "\n",
        "plt.suptitle('Modulus based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6H0QYln6a75U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Stiffness'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Stiffness')\n",
        "plt.suptitle('Stiffness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jA0by-6JbK3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "files_p = processing_procedure(files)\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files_p]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files_p):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Load'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Load')\n",
        "\n",
        "plt.suptitle('Load based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tJll6_92be9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulizing Data\n",
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop('Depth', axis=1) for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering methods\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "# Visualization\n",
        "n_rows = len(scaled_datasets)\n",
        "n_cols = len(clustering_algorithms)\n",
        "\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 15))\n",
        "\n",
        "for row, scaled_data in enumerate(scaled_datasets):\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for col, (algorithm_name, algorithm) in enumerate(clustering_algorithms.items()):\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        axs[row, col].scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        axs[row, col].set_title(f'Dataset {row + 1} using {algorithm_name}')\n",
        "\n",
        "\n",
        "        if row == 0:\n",
        "            axs[row, col].set_xlabel(algorithm_name)\n",
        "        if col == 0:\n",
        "            axs[row, col].set_ylabel(f'Dataset {row + 1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7BGrLWvwdHkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra\n",
        "!pip install fuzzy-c-means"
      ],
      "metadata": {
        "id": "yRxJH-qcoQTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "files_p = processing_procedure(files)\n",
        "# EXCLUDE *******************************************************\n",
        "columns_to_drop = ['Depth', 'X Position', 'Y Position', 'Z Position', 'Load', 'Stiffness'] # please exclude those features that you dont need them\n",
        "#First we do the expriments with Harness alone and then Modulus Alone and then The combination of them\n",
        "datasets = [pd.read_excel(file, engine='openpyxl').drop(columns_to_drop, axis=1, errors='ignore') for file in files_p]\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': silhouette_score,\n",
        "    'Calinski-Harabasz': calinski_harabasz_score,\n",
        "    'Davies-Bouldin': davies_bouldin_score\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, (scaled_data, file_name) in enumerate(zip(scaled_datasets, files_p)):\n",
        "    dataset_label = 'D' + str(index + 1)\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        if len(set(cluster_labels)) > 1:\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                score = metric_func(scaled_data, cluster_labels)\n",
        "                results.append({'Metric': metric_name,\n",
        "                                'Dataset': dataset_label,\n",
        "                                'Method': algorithm_name,\n",
        "                                'Score': score})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "grouped = results_df.groupby(['Metric', 'Dataset', 'Method']).Score.mean().unstack()\n",
        "\n",
        "\n",
        "if os.path.exists(\"HMresults.xlsx\"):\n",
        "    os.remove(\"HMresults.xlsx\")\n",
        "\n",
        "grouped.to_excel(\"HMresults.xlsx\")\n",
        "print(grouped.to_latex())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sF2jU4MxBCl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"HMresults.xlsx\", engine='openpyxl')\n",
        "\n",
        "\n",
        "metric_columns = df.columns.difference(['Metric', 'Dataset'])\n",
        "\n",
        "\n",
        "updated_rows = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "\n",
        "    updated_rows = updated_rows.append(row)\n",
        "\n",
        "\n",
        "    if row['Dataset'] == 'D4':\n",
        "\n",
        "        last_4_rows = df.iloc[idx-3:idx+1][metric_columns]\n",
        "        avg_values = last_4_rows.mean()\n",
        "\n",
        "\n",
        "        avg_row_data = row.to_dict()\n",
        "        avg_row_data.update(avg_values)\n",
        "        avg_row_data['Dataset'] = 'Avg'\n",
        "\n",
        "\n",
        "        updated_rows = updated_rows.append(avg_row_data, ignore_index=True)\n",
        "\n",
        "\n",
        "df_updated = updated_rows.reset_index(drop=True)\n",
        "df_updated.to_excel(\"HMresults2.xlsx\", index=False)\n",
        "\n",
        "\n",
        "print(df_updated)"
      ],
      "metadata": {
        "id": "hSPKo5kcGrfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"HMresults2.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "\n",
        "averages = df.groupby(level=0).mean()\n",
        "\n",
        "\n",
        "for metric in averages.index:\n",
        "    df.loc[(metric, 'Avg'), :] = averages.loc[metric]\n",
        "\n",
        "# Determine rankings for the averages. Higher is better for Silhouette and Calinski-Harabasz,\n",
        "# while lower is better for Davies-Bouldin.\n",
        "metrics = df.index.get_level_values(0).unique()\n",
        "for metric in metrics:\n",
        "    if metric in [\"Silhouette\", \"Calinski-Harabasz\"]:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=False).astype(int)\n",
        "    else:\n",
        "        rank = df.loc[(metric, 'Avg')].rank(ascending=True).astype(int)\n",
        "    df.loc[(metric, 'Rank'), :] = rank\n",
        "\n",
        "\n",
        "order = ['D1', 'D2', 'D3', 'D4', 'Avg', 'Rank']\n",
        "sorted_tuples = sorted(df.index, key=lambda x: (metrics.tolist().index(x[0]), order.index(x[1])))\n",
        "df = df.reindex(sorted_tuples)\n",
        "\n",
        "\n",
        "df.to_excel(\"HMresults.xlsx\")\n",
        "print(df.to_latex())\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "vUvKPOuYKUO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"Mresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "\n",
        "\n",
        "rankings = df.xs('Rank', level=1)\n",
        "\n",
        "\n",
        "colors = {\n",
        "    'Silhouette': 'green',\n",
        "    'Davies-Bouldin': 'blue',\n",
        "    'Calinski-Harabasz': 'orange'\n",
        "}\n",
        "\n",
        "\n",
        "algorithms_order = ['KMeans', 'KMedoids', 'FCM', 'Agglomerative', 'BIRCH', 'DBSCAN', 'OPTICS', 'GMM']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(rankings), figsize=(15, 5))\n",
        "\n",
        "for ax, (metric, data) in zip(axes, rankings.iterrows()):\n",
        "    adjusted_rank = 9 - data.reindex(algorithms_order)  # To make Rank 1 have the highest height\n",
        "    bars = adjusted_rank.plot(kind='bar', ax=ax, title=metric, color=colors[metric], width=0.5)\n",
        "\n",
        "    highest_rank_bar = data.idxmin()\n",
        "\n",
        "\n",
        "    bar_width = bars.patches[0].get_width()\n",
        "    bar_x = bars.patches[adjusted_rank.index.get_loc(highest_rank_bar)].get_x()\n",
        "    bar_height = bars.patches[adjusted_rank.index.get_loc(highest_rank_bar)].get_height()\n",
        "\n",
        "\n",
        "    rect = plt.Rectangle((bar_x - 0.1, 0), bar_width + 0.2, bar_height + 0.1, fill=False, edgecolor='red', linewidth=1.5)\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    ax.set_ylabel('Rank')\n",
        "    ax.set_ylim(0, 9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Di7GXnIJPJ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def kendall_tau_distance(rank_A, rank_B):\n",
        "    n = len(rank_A)\n",
        "    pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n",
        "    disagreements = 0\n",
        "    for x, y in pairs:\n",
        "        a = rank_A[x] - rank_A[y]\n",
        "        b = rank_B[x] - rank_B[y]\n",
        "        if a * b < 0:\n",
        "            disagreements += 1\n",
        "    return disagreements\n",
        "\n",
        "def kemeny_young(rankings):\n",
        "    n = len(rankings[0])\n",
        "    min_distance = float('inf')\n",
        "    best_ranking = None\n",
        "    for candidate in permutations(range(n)):\n",
        "        total_distance = sum(kendall_tau_distance(candidate, rank) for rank in rankings)\n",
        "        if total_distance < min_distance:\n",
        "            min_distance = total_distance\n",
        "            best_ranking = candidate\n",
        "    return best_ranking\n",
        "\n",
        "df = pd.read_excel(\"Hresults.xlsx\", engine='openpyxl', index_col=[0, 1])\n",
        "rankings_df = df.xs('Rank', level=1)\n",
        "methods_order = [\"KMeans\", \"KMedoids\", \"FCM\", \"Agglomerative\", \"BIRCH\", \"DBSCAN\", \"OPTICS\" , \"GMM\"]\n",
        "rankings = [list(row[method] for method in methods_order) for _, row in rankings_df.iterrows()]\n",
        "aggregate_ranking = [methods_order[i] for i in kemeny_young(rankings)]\n",
        "\n",
        "\n",
        "height_dict = {method: 9 - idx for idx, method in enumerate(aggregate_ranking)}\n",
        "\n",
        "\n",
        "bar_heights = [height_dict[method] for method in methods_order]\n",
        "\n",
        "hardness_results_df = pd.DataFrame({\n",
        "    'Clustering Methods': methods_order,\n",
        "    'Rank': [9 - height_dict[method] for method in methods_order]\n",
        "})\n",
        "\n",
        "\n",
        "hardness_results_df = hardness_results_df.sort_values(by='Rank')\n",
        "\n",
        "\n",
        "hardness_results_df.to_excel(\"hardness_ranking_result.xlsx\", index=False)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(range(len(methods_order)), bar_heights, color='blue')\n",
        "ax.set_xticks(range(len(methods_order)))\n",
        "ax.set_xticklabels(methods_order, rotation=90)\n",
        "\n",
        "max_height_idx = bar_heights.index(max(bar_heights))\n",
        "rect_width = 0.95\n",
        "rect_x_start = max_height_idx - (rect_width / 2)\n",
        "rect = Rectangle((rect_x_start, 0), rect_width, max(bar_heights), linewidth=1.5, edgecolor='r', facecolor='none', linestyle='dotted')\n",
        "ax.add_patch(rect)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Clustering Methods')\n",
        "ax.set_ylabel('Rank')\n",
        "ax.set_title('(a) Hardness')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDfAwBWsWuFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the ranking results for hardness, modulus, and hm\n",
        "hardness_df = pd.read_excel('hardness_rank_results.xlsx')\n",
        "modulus_df = pd.read_excel('modulus_rank_results.xlsx')\n",
        "hm_df = pd.read_excel('hm_rank_results.xlsx')\n",
        "\n",
        "# Transpose each dataframe so that the clustering methods become columns\n",
        "hardness_df = hardness_df.T\n",
        "modulus_df = modulus_df.T\n",
        "hm_df = hm_df.T\n",
        "\n",
        "# Extract the ranks for each attribute into a new dataframe\n",
        "data = {\n",
        "    hardness_df.columns[0]: hardness_df.iloc[0].values,\n",
        "    modulus_df.columns[0]: modulus_df.iloc[0].values,\n",
        "    hm_df.columns[0]: hm_df.iloc[0].values\n",
        "}\n",
        "\n",
        "combined_df = pd.DataFrame(data)\n",
        "\n",
        "# Set the index to the names of the clustering methods\n",
        "combined_df.index = hardness_df.index\n",
        "\n",
        "# Transpose the final dataframe so that clustering methods are column headers\n",
        "combined_df = combined_df.transpose()\n",
        "\n",
        "combined_df.to_excel('ready_for_ranking.xlsx')\n",
        "\n"
      ],
      "metadata": {
        "id": "E9rxMGWz0w-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def kendall_tau_distance(rank_A, rank_B):\n",
        "    n = len(rank_A)\n",
        "    pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n",
        "    disagreements = 0\n",
        "    for x, y in pairs:\n",
        "        a = rank_A[x] - rank_A[y]\n",
        "        b = rank_B[x] - rank_B[y]\n",
        "        if a * b < 0:\n",
        "            disagreements += 1\n",
        "    return disagreements\n",
        "\n",
        "def kemeny_young_single(rankings):\n",
        "    n = len(rankings[0])  # Number of methods, as determined by the length of a single ranking\n",
        "    min_distance = float('inf')\n",
        "    best_ranking = None\n",
        "    for candidate in permutations(range(n)):\n",
        "        total_distance = sum(kendall_tau_distance(candidate, rank) for rank in rankings)\n",
        "        if total_distance < min_distance:\n",
        "            min_distance = total_distance\n",
        "            best_ranking = candidate\n",
        "    return best_ranking\n",
        "\n",
        "df = pd.read_excel(\"ready_for_ranking.xlsx\", engine='openpyxl')\n",
        "\n",
        "# Transpose the dataframe to get methods as columns and ranks as rows\n",
        "df = df.transpose()\n",
        "\n",
        "clustering_methods = df.index.tolist()  # Change this to use the index, not columns\n",
        "aggregate_rankings = {}\n",
        "\n",
        "all_rankings = [df[column].tolist() for column in df.columns]\n",
        "aggregated_rank = kemeny_young_single(all_rankings)\n",
        "\n",
        "# Adjust the rank\n",
        "num_methods = len(clustering_methods)\n",
        "adjusted_rank = [num_methods - rank for rank in aggregated_rank]\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(clustering_methods, adjusted_rank, color='blue')\n",
        "ax.set_xticks(clustering_methods)\n",
        "ax.set_xticklabels(clustering_methods, rotation=90)\n",
        "ax.set_ylabel('Adjusted Rank')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gT-GRlQ0_y4D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "name": "evaluation_modulus4.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}