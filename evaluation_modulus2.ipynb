{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mm6396/ClusterComp/blob/main/evaluation_modulus2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHucI84mXoGi"
      },
      "source": [
        "##**Imports Section**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj4a7fr6XoGi"
      },
      "outputs": [],
      "source": [
        "!pip install fuzzy-c-means\n",
        "!pip install scikit-learn-extra\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy import interpolate\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "from fcmeans import FCM\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.cluster import kmeans_plusplus\n",
        "from patsy import dmatrices\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import seaborn as sn\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "file_path = \"test2.xlsx\"\n",
        "sheet_name = \"Test 1\"\n",
        "use_cols = \"A:C\"\n",
        "hard_col_name = \"HARDNESS\"\n",
        "x_col_name = \"X Position\"\n",
        "y_col_name = \"Y Position\"\n",
        "modulus_col_name = \"MODULUS\"\n",
        "NUM_HIST_BINS = 200\n",
        "nulls = False\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeiJcGQeXoGj"
      },
      "source": [
        "##**Variables and Constants**\n",
        "This is the section Setting up any variables or constants used throughout the program. This is where you'll want to set things like the filename and any constants you use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiHibg-UXoGk"
      },
      "outputs": [],
      "source": [
        "xls1 = pd.read_excel('test1.xlsx')\n",
        "xls2 = pd.read_excel('test2.xlsx')\n",
        "\n",
        "xls3 = pd.read_excel('test3.xlsx')\n",
        "xls4 = pd.read_excel('test4.xlsx')\n",
        "\n",
        "#if nulls:\n",
        "  #  dxl = pd.read_excel(xls, sheet_name, usecols=use_cols).iloc[:-1]\n",
        "#else:\n",
        "  #  dxl = pd.read_excel(xls, sheet_name, usecols=use_cols).dropna()\n",
        "#print(xls)\n",
        "corr_matrix1 = xls1.corr()\n",
        "corr_matrix2 = xls2.corr()\n",
        "corr_matrix3 = xls3.corr()\n",
        "corr_matrix4 = xls4.corr()\n",
        "\n",
        "sn.heatmap(corr_matrix1, annot=True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "sn.heatmap(corr_matrix2, annot=True)\n",
        "plt.show()\n",
        "\n",
        "sn.heatmap(corr_matrix3, annot=True)\n",
        "plt.show()\n",
        "\n",
        "sn.heatmap(corr_matrix4, annot=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqPpRYDiXoGl"
      },
      "source": [
        "##**Organizing Data and Making it Numeric**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "implVt7sXoGl",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# If skip_first_row then read every row after the first, otherwise read them all\n",
        "# The hardness values\n",
        "hardness_column = xls4[hard_col_name]\n",
        "# The x values\n",
        "x_column = xls4[x_col_name]\n",
        "# The y values\n",
        "y_column = xls4[y_col_name]\n",
        "modulus_column=xls4[modulus_col_name]\n",
        "# The stiffness column\n",
        "# Asserts that the columns have values\n",
        "assert hardness_column is not None\n",
        "assert x_column is not None\n",
        "assert y_column is not None\n",
        "assert modulus_column is not None\n",
        "\n",
        "hard_df = pd.DataFrame(hardness_column)\n",
        "modulus_df=pd.DataFrame(modulus_column)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(modulus_df.describe())\n",
        "fig=px.box(modulus_df, y='MODULUS')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "fig=px.box(modulus_df, y='MODULUS')\n",
        "fig.show()\n",
        "\n",
        "print(\"hm_df is  :\" ,modulus_df)\n",
        "x_df = pd.DataFrame(x_column)\n",
        "y_df = pd.DataFrame(y_column)\n",
        "\n",
        "# Renames columns of all read data for consistency\n",
        "#hard_df = hard_df.rename(columns={hard_col_name: \"Data\"})\n",
        "\n",
        "#modulus_df=modulus_df.rename(columns={modulus_col_name:\"Data\"})\n",
        "\n",
        "#hm_df=modulus_df.rename(columns={modulus_col_name:\"Data\" ,hard_col_name: \"Data\" })\n",
        "#print(hm_df[\"Data\"])\n",
        "\n",
        "x_df = x_df.rename(columns={x_col_name: \"Data\"})\n",
        "y_df = y_df.rename(columns={y_col_name: \"Data\"})\n",
        "\n",
        "\n",
        "# Makes all read data numeric\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Befor removing outliers"
      ],
      "metadata": {
        "id": "tW7OnK8i38i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(modulus_df)"
      ],
      "metadata": {
        "id": "qXkJX4ej347E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWAPIFusXoGm"
      },
      "source": [
        "##**Data Clean - Setting Nulls** and interpolating\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Highest_allowed_Modulus=modulus_df['MODULUS'].mean() + 3*modulus_df['MODULUS'].std()\n",
        "Lowest_allowed_Modulus=modulus_df['MODULUS'].mean() - 3*modulus_df['MODULUS'].std()\n",
        "\n",
        "\n",
        "print(\"Highest allowed Modulus\",Highest_allowed_Modulus)\n",
        "print(\"Lowest allowed Modulus\",Lowest_allowed_Modulus)\n",
        "\n",
        "\n",
        "#Step-5: Finding the Outliers\n",
        "#new_hm_df11=hm_df[(hm_df['MODULUS'] < Highest_allowed_Modulus) | (hm_df['MODULUS'] > Lowest_allowed_Modulus)].append(hm_df[(hm_df['HARDNESS'] < Highest_allowed_hardness) | (hm_df['HARDNESS'] > Lowest_allowed_hardness)])\n",
        "#print(new_hm_df11)\n",
        "#plot outliers\n",
        "\n",
        "#plt.scatter(new_hm_df11['HARDNESS'],new_hm_df11['MODULUS'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "new_modulus_df = np.where(\n",
        "    modulus_df ['MODULUS']> Highest_allowed_Modulus,\n",
        "    modulus_df['MODULUS'].mean() , modulus_df['MODULUS'])\n",
        "new_modulus_df=np.where(\n",
        "        modulus_df['MODULUS']< Lowest_allowed_Modulus,\n",
        "       modulus_df['MODULUS'].mean(),\n",
        "        modulus_df['MODULUS']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(new_modulus_df)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-QNcO_aVut94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyKr5rgrXoGp"
      },
      "source": [
        "#**Clustering the Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN clustering"
      ],
      "metadata": {
        "id": "75h1OA4Wd76r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "import time\n",
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number !\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "\n",
        "print('\\n <><><><><><><><><><> \"  After cleaning  \"<><><><><><><><><><><>\\n\\n')\n",
        "print(new_modulus_df)\n",
        "\n",
        "print(np.shape(new_modulus_df))\n",
        "db = DBSCAN(eps=0.69, min_samples=100).fit(new_modulus_df['MODULUS'].values.reshape(-1,1))\n",
        "labels = db.labels_\n",
        "\n",
        "print(\"These are DBSCAN_labels: \\n\" , labels, len(labels)  , new_modulus_df)\n",
        "\n",
        "\n",
        "print('Sihhuette score ', num_clusters ,' is' , silhouette_score(new_modulus_df,labels))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, labels))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, labels))\n",
        "\n",
        "print('\\n <><><><><><><><><><> \"  Before \"<><><><><><><><><><><>\\n')\n",
        "modulus_df = modulus_df.dropna()\n",
        "print(modulus_df)\n",
        "db = DBSCAN(eps=0.69, min_samples=100).fit(modulus_df[:])\n",
        "labels = db.labels_\n",
        "from collections import Counter\n",
        "Counter(labels)\n",
        "\n",
        "print(\"These are DBSCAN_labels: \\n\" , labels, len(labels)  , modulus_df)\n",
        "\n",
        "print('Sihhuette score ', num_clusters ,' is' , silhouette_score(modulus_df,labels))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, labels))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, labels))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ugcf8qfmhMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahokFC07zYxv"
      },
      "source": [
        "# **This is Fuzzy c-means**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IMOC33szqr7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from fcmeans import FCM\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number !\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "new_hm_df= new_modulus_df.dropna()\n",
        "fcm = FCM(n_clusters= 3, Fuzzifier= 2)\n",
        "print(new_hm_df)\n",
        "fcm.fit(new_modulus_df.to_numpy())\n",
        "\n",
        "#fcm_centers_cleaned = fcm.centers\n",
        "\n",
        "fcm_labels_cleaned = fcm.predict(new_modulus_df.to_numpy())\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(new_modulus_df,fcm_labels_cleaned))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, fcm_labels_cleaned))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, fcm_labels_cleaned))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n ')\n",
        "modulus_df= modulus_df.dropna()\n",
        "fcm = FCM(n_clusters=num_clusters , Fuzzifier= 2)\n",
        "fcm.fit(new_modulus_df.to_numpy())\n",
        "\n",
        "#fcm_centers_cleaned = fcm.centers\n",
        "print(\"fcm centers are :\",fcm_centers_cleaned)\n",
        "fcm_labels_cleaned = fcm.predict(modulus_df.to_numpy())\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(modulus_df,fcm_labels_cleaned))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, fcm_labels_cleaned))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, fcm_labels_cleaned))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeM71IwHrGA"
      },
      "source": [
        "#**This is KMeans clustering**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number!\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "k_means = KMeans(n_clusters=num_clusters, random_state=0, tol=0.0001, verbose=0.5, algorithm='auto').fit(new_modulus_df)\n",
        "k_means_labels_cleaned = k_means.labels_\n",
        "\n",
        "\n",
        "\n",
        "print('Sihhuette score for number of couters ' , num_clusters ,' is' , silhouette_score(new_modulus_df,k_means.labels_))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(new_modulus_df, k_means_labels_cleaned))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, k_means_labels_cleaned))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "k_means = KMeans(n_clusters=num_clusters, random_state=0 , tol=0.0001, verbose=0.5, algorithm='auto').fit(hm_df)\n",
        "k_means_labels_cleaned = k_means.labels_\n",
        "\n",
        "\n",
        "\n",
        "print('Sihhuette score for number of clusters ' , num_clusters ,' is' , silhouette_score(modulus_df,k_means.labels_))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(modulus_df, k_means_labels_cleaned))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, k_means_labels_cleaned))\n"
      ],
      "metadata": {
        "id": "FGiNEDdss7S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG0rbXpKcfXG"
      },
      "source": [
        "#**This is GuassianMixture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLUqp-bnVchJ"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"The number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number at all! This is a string. go and try again. Better luck next time!\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "gmm= GaussianMixture(n_components= num_clusters)\n",
        "gmm.fit(new_modulus_df)\n",
        "\n",
        "gmm_results = gmm.predict(new_modulus_df)\n",
        "\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(new_modulus_df,gmm_results))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, gmm_results))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, gmm_results))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "gmm= GaussianMixture(n_components= num_clusters)\n",
        "gmm.fit(modulus_df)\n",
        "\n",
        "gmm_results = gmm.predict(modulus_df)\n",
        "\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(modulus_df,gmm_results))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, gmm_results))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, gmm_results))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**This is Agglomorative Clustering**"
      ],
      "metadata": {
        "id": "rC53WjZ13Wgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0H371AI6Ua7"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering # this line of code imports AgglomerativeClustering model from sk-learn\n",
        "'''\n",
        "we need to create an AgglomerativeClustering object, and in it, we pass the following parameters:\n",
        "n_cluster= 5, the number of clusters our model should return\n",
        "affinity=euclidean, specify metric to be used to calculate distances\n",
        "linkage= ward to regulate how distance calculation will be carried out between different clusters.\n",
        "'''\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "num_clusters = 3\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "Agg_hc = AgglomerativeClustering(n_clusters = num_clusters,  linkage = 'complete')\n",
        "y_hc = Agg_hc.fit_predict(new_modulus_df)# model fitting on the dataset\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(new_modulus_df,y_hc))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(new_modulus_df, y_hc))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, y_hc))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "Agg_hc = AgglomerativeClustering(n_clusters = num_clusters , linkage = 'complete')\n",
        "y_hc = Agg_hc.fit_predict(modulus_df)# model fitting on the dataset\n",
        "\n",
        "\n",
        "\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(modulus_df,y_hc))\n",
        "\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, y_hc))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, y_hc))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectral clustering"
      ],
      "metadata": {
        "id": "BK0-ApcsJS_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**This is Birch Clustering**"
      ],
      "metadata": {
        "id": "bLAz0VF43fnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "import time\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "birch_clust=Birch(branching_factor=100, threshold =0.25).fit(new_modulus_df)\n",
        "birch_results = birch_clust.predict(new_modulus_df)\n",
        "\n",
        "clustered_df = pd.DataFrame(birch_results)\n",
        "#clustered_df.columns = [\"Data\"]\n",
        "num_clusters = 3\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(new_modulus_df,birch_results))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, birch_results))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, birch_results))\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "\n",
        "birch_clust=Birch(branching_factor=100, threshold =0.25).fit(modulus_df)\n",
        "birch_results = birch_clust.predict(modulus_df)\n",
        "\n",
        "clustered_df = pd.DataFrame(birch_results)\n",
        "#clustered_df.columns = [\"Data\"]\n",
        "num_clusters = 3\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(modulus_df,birch_results))\n",
        "print('C-H index  ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulus_df, birch_results))\n",
        "print('D-B index  ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, birch_results))\n",
        "\n"
      ],
      "metadata": {
        "id": "rWoslMsT3gd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**This is Optics Clustering**"
      ],
      "metadata": {
        "id": "bvvxkfVK-Jue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "import time\n",
        "num_clusters =3\n",
        "obj = time.gmtime(0)\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "optics_clustering = OPTICS(min_samples=2 , max_eps = 0.9).fit(new_modulus_df)\n",
        "\n",
        "optics_labels=optics_clustering.labels_\n",
        "optics_results = optics_clustering.fit_predict(new_modulus_df)\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(new_modulus_df,optics_labels))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(new_modulus_df, optics_labels))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, optics_labels))\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "optics_clustering = OPTICS(min_samples=2 , max_eps = 0.9).fit(modulus_df)\n",
        "optics_clustering_labels=optics_clustering.labels_\n",
        "optics_results = optics_clustering.fit_predict(modulus_df)\n",
        "\n",
        "print('Sihhuette score ' , num_clusters ,' is' , silhouette_score(modulus_df,optics_results))\n",
        "print('C-H index ' , num_clusters , 'is' , calinski_harabasz_score(modulus_df, optics_results))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, optics_results))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egJ_z79d-KAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implementing kmedoid clustering ***"
      ],
      "metadata": {
        "id": "JS4UZnI2zIuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "     input_user= input('please enter the number of clusters:')\n",
        "     try:\n",
        "       num_clusters = int(input_user)\n",
        "       print(\"number of clusters is \", num_clusters)\n",
        "     except ValueError:\n",
        "       print(\"This is not a valid number, It is not a number !\")\n",
        "     else:\n",
        "       print(\" Your cluster number is: \", num_clusters)\n",
        "       break\n",
        "start_time = round(time.time()*1000)\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" After  \"<><><><><><><><><><> \\n\\n')\n",
        "k_medoids= KMedoids(n_clusters=num_clusters).fit(new_modulus_df)\n",
        "k_medoids_labels_cleaned = k_medoids.labels_\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(new_modulus_df,k_medoids_labels_cleaned))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(new_modulus_df, k_medoids_labels_cleaned))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(new_modulus_df, k_medoids_labels_cleaned))\n",
        "\n",
        "\n",
        "print('\\n<><><><><><><><><><><> \" Before  \"<><><><><><><><><><> \\n\\n')\n",
        "k_medoids= KMedoids(n_clusters=num_clusters).fit(modulus_df)\n",
        "k_medoids_labels_cleaned = k_medoids.labels_\n",
        "print('Sihhuette score  ' , num_clusters ,' is' , silhouette_score(modulus_df,k_medoids_labels_cleaned))\n",
        "print('C-H index ' , num_clusters , 'is' , metrics.calinski_harabasz_score(modulu_df, k_medoids_labels_cleaned))\n",
        "print('D-B index ' , num_clusters , 'is' , davies_bouldin_score(modulus_df, k_medoids_labels_cleaned))\n",
        "\n"
      ],
      "metadata": {
        "id": "5l6_0qnyzH_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['HARDNESS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Hardness')\n",
        "\n",
        "plt.suptitle('Hardness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p5RBL3wOZ5EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['MODULUS'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Modulus')\n",
        "\n",
        "plt.suptitle('Modulus based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6H0QYln6a75U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Stiffness'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Stiffness')\n",
        "\n",
        "plt.suptitle('Stiffness based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jA0by-6JbK3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [\"test1.xlsx\", \"test2.xlsx\", \"test3.xlsx\", \"test4.xlsx\"]\n",
        "\n",
        "\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(datasets), figsize=(15, 5), sharex=True, sharey=True)\n",
        "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
        "\n",
        "for ax, df, file in zip(axs, datasets, files):\n",
        "    sc = ax.scatter(df['X Position'], df['Y Position'], c=df['Load'], cmap='viridis')\n",
        "    ax.set_title(file)\n",
        "    ax.set_xlabel('X Position')\n",
        "    if ax == axs[0]:\n",
        "        ax.set_ylabel('Y Position')\n",
        "\n",
        "fig.colorbar(sc, cax=cbar_ax, label='Load')\n",
        "\n",
        "plt.suptitle('Load based on X and Y Position')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tJll6_92be9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulizing Data\n",
        "!pip install scikit-learn-extra\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "files = [ \"test3.xlsx\"]\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "# Normalize/Standardize the datasets\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering algorithms\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "for index, scaled_data in enumerate(scaled_datasets, 1):\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        plt.xlabel('First Principal Component')\n",
        "        plt.ylabel('Second Principal Component')\n",
        "        plt.title(f'PCA Cluster Visualization for Dataset {index} using {algorithm_name}')\n",
        "        plt.colorbar()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7BGrLWvwdHkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install fuzzy-c-means\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the datasets\n",
        "files = [\"test3.xlsx\"]\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "# Normalize/Standardize the datasets\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "# Define clustering algorithms\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "# Apply PCA and visualize each dataset with each clustering algorithm\n",
        "for index, scaled_data in enumerate(scaled_datasets, 1):\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        plt.xlabel('First Principal Component')\n",
        "        plt.ylabel('Second Principal Component')\n",
        "        plt.title(f'PCA Cluster Visualization for Dataset {index} using {algorithm_name}')\n",
        "        plt.colorbar()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "oHCGbRWkiNDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from fcmeans import FCM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the datasets\n",
        "files = [\"test3.xlsx\"]\n",
        "datasets = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
        "\n",
        "\n",
        "scaled_datasets = [StandardScaler().fit_transform(df) for df in datasets]\n",
        "\n",
        "\n",
        "clustering_algorithms = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'DBSCAN': DBSCAN(eps=0.5),\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "    'OPTICS': OPTICS(),\n",
        "    'KMedoids': KMedoids(n_clusters=3, random_state=42),\n",
        "    'GMM': GaussianMixture(n_components=3, random_state=42),\n",
        "    'BIRCH': Birch(n_clusters=3),\n",
        "    'FCM': FCM(n_clusters=3)\n",
        "}\n",
        "\n",
        "\n",
        "for index, scaled_data in enumerate(scaled_datasets, 1):\n",
        "    pca = PCA(n_components=3)\n",
        "    data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    for algorithm_name, algorithm in clustering_algorithms.items():\n",
        "        if algorithm_name == 'GMM':\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "        elif algorithm_name == 'FCM':\n",
        "            algorithm.fit(scaled_data)\n",
        "            cluster_labels = algorithm.u.argmax(axis=1)\n",
        "        else:\n",
        "            cluster_labels = algorithm.fit_predict(scaled_data)\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 6))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        scatter = ax.scatter(data_pca[:, 0], data_pca[:, 1], data_pca[:, 2], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
        "        ax.set_xlabel('First Principal Component')\n",
        "        ax.set_ylabel('Second Principal Component')\n",
        "        ax.set_zlabel('Third Principal Component')\n",
        "        ax.set_title(f'PCA Cluster Visualization for Dataset {index} using {algorithm_name}')\n",
        "        fig.colorbar(scatter)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "wN7iSDkBmzrM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}